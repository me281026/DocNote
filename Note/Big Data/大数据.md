# 大数据

## 1.Hadoop

### 1.1.HDFS

#### 1.1.1.Namenode 和 Datanode

HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。

#### 1.1.2.HDFS架构

![hdfs](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)

#### 1.1.3.Namespace

HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。

Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由Namenode保存的。

#### 1.1.4.Data Replication

HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。

Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。

![数据复制](https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsdatanodes.gif)

**存放机制:** 副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。

在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。

#### 1.1.5.组件功能

![组件](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632294496.jpg)

#### 1.1.6.HDFS写流程

![写](https://pic4.zhimg.com/v2-6de9f191ab1dce72dcda3f5aa2d3b33f_b.jpg)

1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。
2. NameNode返回是否可以上传。
3. 客户端请求第一个 Block上传到哪几个DataNode服务器上。
4. NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。
5. 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。
6. dn1、dn2、dn3逐级应答客户端。
7. 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。
8. 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。

---
NameNode：可以理解为DataNode管理器
DataNode：存储块数据，默认128M为一块

**网络拓扑-节点距离计算** :在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？

节点距离：两个节点到达最近的共同祖先的距离总和。

不好理解的就是共同祖先啥意思，可以理解为上级节点，节点等级如下
【数据中心（集群d）--->  机架r ---> 具体节点n】
举个例子：计算节点d1/r1/n1到节点d1/r1/n2的节点距离
确认共同祖先为r1，节点n1到它的距离为1，节点n2到它的距离也为1，两者和为2
所以它们之间的节点距离就是2

![距离计算](https://pic4.zhimg.com/v2-7fdaf587e5b47b8deede56d235c229f3_b.jpg)

**机架感知** :副本节点的位置的选择就行

![位置选择](https://pic2.zhimg.com/v2-44f72ca2fc3214c48d02958241177d89_b.jpg)

---

#### 1.1.7.HDFS读流程

![读](https://pic3.zhimg.com/v2-811b2a0e2bc91542e195da86d299a9a2_b.jpg)

1. 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

#### 1.1.8.HDFS的HA

![HA](https://img-blog.csdn.net/20180821142402638?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzI4OTk3NjU1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Namenode在HDFS中是一个非常重要的组件，相当于HDFS文件系统的心脏，在显示分布式集群环境中，还是会有可能出现Namenode的崩溃或各种意外。所以，高可用模式就体现出作用了。
namenode HA配置大概流程：

- 在启动namenode之前，需要启动hadoop2.x中新引入的（QJM）Quorum Journal Manager，QJM主要用来管理namenode之间的数据同步，当active namenode数据更新时会传递给QJM，QJM在所有的namenode之间同步，最后QJM将active namenode 更新的数据同步到了standby namenode中。
- 启动多个namenode时，并配置namenode的主机地址，还要配置隔离机制，因为容易出现SB（split-brain）状况，所谓的sb状况意思就是当多个namenode正常状态时，一台active，多台standby。如果某段时间因为网络等非namenode自身关系导致namenode间交流阻断了，这样容易出现多台active的设备，容易抢占资源等。
- 引入zookeeper来对namenode进行监听，因为在一般情况下，active 的namenode崩溃了的话，需要人工切换standby Namenode为active。非常不人性化。通过zookeeper可以监听多个namenode，当active namenode崩溃的话，zookeeper监听到后马上通知zookeeper的leader进行主备选举，在standby namenode中选举出一台，并将它置为active模式替换崩溃的namenode。

#### 1.1.9.小文件

Hadoop中的小文件一般是指明显小于HDFS的block size（默认128M，一般整数倍配置如256M）的文件。但需要注意，HDFS上的有些小文件是不可避免的，比如jar、临时缓存文件等。但当小文件数量变的"海量"，以至于Hadoop集群中存储了大量的小文件，就需要对小文件进行处理，而处理的目标是让文件大小尽可能接近HDFS的block size大小或者整数倍。

- 明显小于block size文件的80%
- 129M:128M+1M

##### Hadoop小文件带来的问题

1. 众所周知，在HDFS中数据和元数据分别由DataNode和NameNode负责，这些元数据每个对象一般占用大约150个字节。大量的小文件相对于大文件会占用大量的NameNode内存。对NameNode内存管理产生巨大挑战，此外对JVM稳定性也有影响如GC。
2. 当NameNode重启时，它需要将文件系统元数据从本地磁盘加载到内存中。如果NameNode的元数据很大，重启速度会非常慢。
3. 一般来说，NameNode会不断跟踪并检查集群中每个block块的存储位置。这是通过DataNode的定时心跳上报其数据块来实现的。数据节点需要上报的block越多，则也会消耗越多的网络带宽/时延。
4. 更多的文件意味着更多的读取请求需要请求NameNode，这可能最终会堵塞NameNode的容量，增加RPC队列和处理延迟，进而导致性能和响应能力下降。
5. 对计算引擎如Spark、MapReduce性能造成负面影响。以MapReduce（以下简称MR）为例，大量小文件意味着大量的磁盘IO，磁盘IO通常是MR性能的最大瓶颈之一，在HDFS中对于相同数量的数据，一次大的顺序读取往往优于几次随机读取的性能。如果可以将数据存储在较少，而更大的一些block中，可以降低磁盘IO的性能影响。除了磁盘IO，还有内部任务的划分、资源分配等

##### Hadoop小文件是怎么来的

一个Hadoop集群中存在小文件的可能原因如下：

1. 流式任务（如spark streaming/flink等实时计算框架）在做数据处理时，无论是纯实时还是基于batch的准实时，在小的时间窗口内都可能产生大量的小文件。此外对于Spark任务如果过度并行化，每个分区一个文件，产生的文件也可能会增多
2. Hive分区表的过度分区这里的过度分区是指Hive分区表的每个分区数据量很小（比如小于HDFS block size）的Hive表。那么Hive Metastore Server调用开销会随着表拥有的分区数量而增加，影响性能。此时，要衡量数据量重新进行表结构设计（如减少分区粒度）。
3. 数据源有大量小文件，未做处理直接迁移到Hadoop集群。
4. 对于计算引擎处理任务，以MR为例。大量的map和reduce task存在。在HDFS上生成的文件基本上与map数量（对于Map-Only作业）或reduce数量（对于MR作业）成正比。此外，MR任务如果未设置合理的reduce数或者未做限制，每个reduce都会生成一个独立的文件。对于数据倾斜，导致大部分的数据都shuffle到一个或几个reduce，然后其他的reduce都会处理较小的数据量并输出小文件。对于Spark任务，过度并行化也是导致小文件过多的原因之一。在Spark作业中，根据写任务中提到的分区数量，每个分区会写一个新文件。这类似于MapReduce框架中的每个reduce任务都会创建一个新文件。Spark分区越多，写入的文件就越多。控制分区的数量来减少小文件的生成。

### 1.2.SQL On Hadoop

#### 1.2.1.SQL和存储

sql ==> 对应执行引擎的作业:MapReduce/Spark/Tez

Impala:内存
Presto:
Drill:
Phoenix:HBase(rowkey)
Spark SQL:

MetaStore:存储元数据,框架之间是共享元数据信息的

行式存储/列式存储

![C/R](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632376364(1).jpg)

#### 1.2.2.调优策略

1. 架构
   - 分表
   - 分区表(partition)
   - 中间结果集
   - 压缩
2. 语法
   - 排序:order by/partition by/distribute by/cluster by
   - 控制输出(reduce/partition/task)数量
   - join(普通join/map join)
   - 执行计划
3. 执行
   - 推测执行
   - 并行执行
   - JVM重用

##### 1. 架构

**分表:**

![分表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377124(1).jpg)

**分区表:**

![分区表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377381(1).jpg)

**中间结果集:**

![中间结果](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377645(1).jpg)

**压缩:**

![压缩](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377929(1).jpg)

![压缩详解](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632378025(1).jpg)

##### 2. 语法

**Order by / Sort by:**

Hive QL 中的\* ORDER BY 语法类似于 SQL 语言中的 ORDER BY \*语法。

---

colOrder: ( ASC | DESC )
colNullOrder: (NULLS FIRST | NULLS LAST)           -- (Note: Available in Hive 2.1.0 and later)
orderBy: ORDER BY colName colOrder? colNullOrder? (',' colName colOrder? colNullOrder?)*
query: SELECT expression (',' expression)* FROM src orderBy

---

order by子句中有一些限制。在严格模式下(即hive.mapred.mode = strict)，必须在 order by 子句后跟一个limit子句。如果将 hive.mapred.mode 设置为 nonstrict，则没有 limit 子句。原因是为了强加所有结果的总 Sequences，必须有一个减速器才能对最终输出进行排序。如果输出中的行数太大，则单个减速器可能需要很长时间才能完成。

对于 Hive 0.11.0 到 2.1.x，将hive.groupby.orderby.position.alias设置为 true(默认值为 false)。

对于 Hive 2.2.0 和更高版本，默认情况下hive.orderby.position.alias为 true。

SORT BY 语法类似于 SQL 语言中的 ORDER BY *语法。

---

colOrder: ( ASC | DESC )
sortBy: SORT BY colName colOrder? (',' colName colOrder?)*
query: SELECT expression (',' expression)* FROM src sortBy

---

Hive 使用\* SORT BY *中的列对行进行排序，然后再将行 Importing 到 reducer。排序 Sequences 将取决于列类型。如果列是数字类型，则排序 Sequences 也是数字 Sequences。如果列是字符串类型，则排序 Sequences 将是字典 Sequences。

在Hive 3.0.0及更高版本中，优化器将删除subqueries和views中的无限制排序。要禁用它，请将hive.remove.orderby.in.subquery设置为 false。

Hive 支持\* SORT BY *，可对每个 reducer 的数据进行排序。 “ order by”和“ sort by”之间的区别在于，前者保证输出中的总 Sequences，而后者仅保证精简器中行的排序。如果存在多个减速器，则“排序依据”可能会给出部分排序的最终结果。

注意：关于单个列的单独 SORT BY 与 CLUSTER BY 之间的区别可能会造成混淆。不同之处在于，如果有多个 reducer 分区，则 CLUSTER BY 按字段划分，而 SORT BY 则是随机划分，以便在 reducer 上均匀地分布数据(和负载)。

基本上，每个 reducer 中的数据将根据用户指定的 Sequences 进行排序。

**Distribute by/Cluster by:**

Cluster By 和 Distribute By *主要与Transform/Map-Reduce Scripts一起使用。但是，如果需要对查询的输出进行分区和排序以用于后续查询，有时在 SELECT 语句中很有用。

Cluster By 是 Distribute By 和 Sort By *的快捷方式。

Hive 使用\* Distribute By 中的列在reducer之间分配行。具有相同 Distribute By 列的所有行将进入相同的 reducer。但是， Distribute By *不保证分布式键上的聚类或排序属性。

---

例如，我们将以下 5 行的* x 分配给 2 个 reducer：

x1
x2
x4
x3
x1

==reducer 1==

x1
x2
x1

==reducer 2==

x4
x3
请注意，具有相同键 x1 的所有行都可以保证分配给同一reducer(在这种情况下为reducer 1)，但不能保证它们聚集在相邻位置。

相反，如果我们使用\* Cluster By x *，则两个化简器将进一步对 x 上的行进行排序：

==reducer 1==

x1
x1
x2

==reducer 2==

x3
x4
用户可以指定\* Distribute By 和 Sort By 来代替 Cluster By *，因此分区列和排序列可以不同。通常的情况是分区列是排序列的前缀，但这不是必需的。

```SQL
SELECT col1, col2 FROM t1 CLUSTER BY col1
SELECT col1, col2 FROM t1 DISTRIBUTE BY col1

SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC
FROM (
    FROM pv_users
    MAP ( pv_users.userid, pv_users.date )
    USING 'map_script'
    AS c1, c2, c3
    DISTRIBUTE BY c2
    SORT BY c2, c1) map_output
  INSERT OVERWRITE TABLE pv_users_reduced
    REDUCE ( map_output.c1, map_output.c2, map_output.c3 )
    USING 'reduce_script'
    AS date, count;
```

---

**总结:**

![区别](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632463246(1).jpg)

**执行计划(EXPLAIN):**

Hive 提供了一个EXPLAIN命令，该命令显示了查询的执行计划。该语句的语法如下：

EXPLAIN [EXTENDED|CBO|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE] query

HIVE 0.14.0 通过HIVE-5961支持AUTHORIZATION。 Hive 2.3.0 通过HIVE-11394支持VECTORIZATION。 Hive 3.2.0 通过HIVE-17683支持LOCKS。

AST 已从HIVE-13533中的 EXPLAIN EXTENDED 中删除，并在HIVE-15932中作为单独的命令恢复。

在EXPLAIN语句中使用EXTENDED会产生有关计划中运算符的更多信息。这通常是物理信息，例如文件名。

Hive 查询将转换为阶段序列(它更是有向非循环图)。这些阶段可以是 map/reduce 阶段，或者甚至可以是执行元存储或文件系统操作(如移动和重命名)的阶段。说明输出分为三个部分：

查询的抽象语法树

计划不同阶段之间的依赖性

每个阶段的描述

这些阶段本身的描述显示了一系列操作符，以及与操作符相关联的元数据。元数据可能包含诸如 FilterOperator 的过滤器表达式或 SelectOperator 的选择表达式或 FileSinkOperator 的输出文件名之类的内容。

**JOIN:**

LEFT SEMI JOIN 以有效的方式实现了不相关的 IN/EXISTS 子查询语义。从 Hive 0.13 开始，使用subqueries支持 IN/NOT IN/EXISTS/NOT EXISTS 运算符，因此这些 JOIN 中的大多数不再需要手动执行。使用 LEFT SEMI JOIN 的限制是只能在连接条件(ON 子句)中引用右侧表，而不能在 WHERE 或 SELECT 子句等中引用。

如果除一个要连接的表之外的所有表都很小，则可以将其作为仅 Map 作业执行。查询

```SQL

SELECT /*+ MAPJOIN(b) */ a.key, a.value
  FROM a JOIN b ON a.key = b.key

不需要reduce。对于 A 的每个 Map 器，B 都会被完全读取。
```

不支持以下内容。

union 后跟一个 MapJoin

横向视图后跟一个 MapJoin

减少接收器(分组依据/加入/排序依据/集群依据/分发依据)，其次是 MapJoin

MapJoin 之后是 union

MapJoin，然后加入

MapJoin 其次是 MapJoin

配置变量 hive.auto.convert.join(如果设置为 true)会在运行时自动将联接转换为 mapjoins，应使用它代替 mapjoin 提示。 mapjoin 提示仅应用于以下查询。

如果对所有 Importing 进行了存储分区或排序，则该联接应转换为存储分区的 Map 端连接或存储分区的排序合并联接。

考虑在不同的键上使用多个 mapjoin 的可能性：

```SQL
select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM
  ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM
    bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)
  ) firstjoin
  JOIN
  smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo)
```

不支持上述查询。如果没有 mapjoin 提示，则以上查询将作为 2 个仅 map 作业执行。如果用户事先知道 Importing 足够小以适合内存，则可以使用以下可配置参数来确保查询在单个 map-reduce 作业中执行。

hive.auto.convert.join.noconditionaltask-Hive 是否启用基于 Importing 文件大小的关于将普通联接转换为 mapjoin 的优化。如果启用此参数，并且 n 向联接的表/分区的 n-1 个大小的总和小于指定的大小，则该联接将直接转换为 mapjoin(没有条件任务)。

hive.auto.convert.join.noconditionaltask.size-如果关闭了 hive.auto.convert.join.noconditionaltask，则此参数不起作用。但是，如果启用了该连接，并且 n 向联接的表/分区的 n-1 个大小的总和小于此大小，则该联接将直接转换为 mapjoin(没有条件任务)。默认值为 10MB。

Hive 会自动识别各种用例并对其进行优化。 Hive 0.11 改进了以下情况的优化程序：

在一侧适合内存的位置连接。在新的优化中：

将该那一侧作为哈希表加载到内存中

只需要扫描较大的表

事实表在内存中的占用空间较小

Star-schema joins

在许多情况下，不再需要提示。

Map 联接由优化器自动拾取。

![JOIN](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632465053(1).jpg)

##### 3. 运行层面的调优

**推测执行:**

hive.mapred.reduce.tasks.speculative.execution
默认值：true

添加于：Hive0.5.0

是否应该对减速器进行投机执行。

![推测执行](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150517.bmp)

**并行执行:**

hive.exec.parallel
默认值：false

添加于：Hive0.5.0

是否并行执行作业。适用于可以并行运行的 MapReduce 作业，例如，在连接之前处理不同源表的作业。从Hive 0.14开始，也适用于可并行运行的移动任务，例如，在多插入期间移动文件以插入目标。

hive.exec.parallel.thread.number
默认值：8

添加于：Hive0.6.0

最多可以并行执行多少个作业。

![并行执行](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150835.bmp)

**JVM重用:**

Hadoop与JVM重用对应的参数是mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM。

比如在集群中配置每个slave节点最多同时运行16个map和2个reduce。那slave节点会启动最多16个JVM用于map。

为每个task启动一个新的JVM将耗时1秒左右，对于运行时间较长（比如1分钟以上）的job影响不大，但如果都是时间很短的task，那么频繁启停JVM会有开销。

如果我们想使用JVM重用技术来提高性能，那么可以将mapred.job.reuse.jvm.num.tasks设置成大于1的数。这表示属于同一job的顺序执行的task可以共享一个JVM，也就是说第二轮的map可以重用前一轮的JVM，而不是第一轮结束后关闭JVM，第二轮再启动新的JVM。

那么最多一个JVM能顺序执行多少个task才关闭呢？这个值就是mapred.job.reuse.jvm.num.tasks。如果设置成-1，那么只要是同一个job的task（无所谓多少个），都可以按顺序在一个JVM上连续执行。

如果task属于不同的job，那么JVM重用机制无效，不同job的task需要不同的JVM来运行。

JVM重用技术不是指同一Job的两个或两个以上的task可以同时运行于同一JVM上，而是排队按顺序执行。

一个tasktracker最多可以同时运行的task数目由mapred.tasktracker.map.tasks.maximum和mapred.tasktracker.reduce.tasks.maximum

决定，并且这两个参数在mapred-site.xml中设置。其他方法，如在JobClient端通过命令行-Dmapred.tasktracker.map.tasks.maximum=number或者conf.set("mapred.tasktracker.map.tasks.maximum","number")设置都是无效的。

![jvm优化](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150858.bmp)
