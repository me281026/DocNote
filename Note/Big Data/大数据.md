# 大数据

## 1.Hadoop

### 1.1.HDFS

#### 1.1.1.Namenode 和 Datanode

HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。

#### 1.1.2.HDFS架构

![hdfs](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)

#### 1.1.3.Namespace

HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。

Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由Namenode保存的。

#### 1.1.4.Data Replication

HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。

Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。

![数据复制](https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsdatanodes.gif)

**存放机制:** 副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。

在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。

#### 1.1.5.组件功能

![组件](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632294496.jpg)

#### 1.1.6.HDFS写流程

![写](https://pic4.zhimg.com/v2-6de9f191ab1dce72dcda3f5aa2d3b33f_b.jpg)

1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。
2. NameNode返回是否可以上传。
3. 客户端请求第一个 Block上传到哪几个DataNode服务器上。
4. NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。
5. 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。
6. dn1、dn2、dn3逐级应答客户端。
7. 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。
8. 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。

---
NameNode：可以理解为DataNode管理器
DataNode：存储块数据，默认128M为一块

**网络拓扑-节点距离计算** :在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？

节点距离：两个节点到达最近的共同祖先的距离总和。

不好理解的就是共同祖先啥意思，可以理解为上级节点，节点等级如下
【数据中心（集群d）--->  机架r ---> 具体节点n】
举个例子：计算节点d1/r1/n1到节点d1/r1/n2的节点距离
确认共同祖先为r1，节点n1到它的距离为1，节点n2到它的距离也为1，两者和为2
所以它们之间的节点距离就是2

![距离计算](https://pic4.zhimg.com/v2-7fdaf587e5b47b8deede56d235c229f3_b.jpg)

**机架感知** :副本节点的位置的选择就行

![位置选择](https://pic2.zhimg.com/v2-44f72ca2fc3214c48d02958241177d89_b.jpg)

---

#### 1.1.7.HDFS读流程

![读](https://pic3.zhimg.com/v2-811b2a0e2bc91542e195da86d299a9a2_b.jpg)

1. 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

#### 1.1.8.HDFS的HA

![HA](https://img-blog.csdn.net/20180821142402638?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzI4OTk3NjU1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Namenode在HDFS中是一个非常重要的组件，相当于HDFS文件系统的心脏，在显示分布式集群环境中，还是会有可能出现Namenode的崩溃或各种意外。所以，高可用模式就体现出作用了。
namenode HA配置大概流程：

- 在启动namenode之前，需要启动hadoop2.x中新引入的（QJM）Quorum Journal Manager，QJM主要用来管理namenode之间的数据同步，当active namenode数据更新时会传递给QJM，QJM在所有的namenode之间同步，最后QJM将active namenode 更新的数据同步到了standby namenode中。
- 启动多个namenode时，并配置namenode的主机地址，还要配置隔离机制，因为容易出现SB（split-brain）状况，所谓的sb状况意思就是当多个namenode正常状态时，一台active，多台standby。如果某段时间因为网络等非namenode自身关系导致namenode间交流阻断了，这样容易出现多台active的设备，容易抢占资源等。
- 引入zookeeper来对namenode进行监听，因为在一般情况下，active 的namenode崩溃了的话，需要人工切换standby Namenode为active。非常不人性化。通过zookeeper可以监听多个namenode，当active namenode崩溃的话，zookeeper监听到后马上通知zookeeper的leader进行主备选举，在standby namenode中选举出一台，并将它置为active模式替换崩溃的namenode。

#### 1.1.9.小文件

Hadoop中的小文件一般是指明显小于HDFS的block size（默认128M，一般整数倍配置如256M）的文件。但需要注意，HDFS上的有些小文件是不可避免的，比如jar、临时缓存文件等。但当小文件数量变的"海量"，以至于Hadoop集群中存储了大量的小文件，就需要对小文件进行处理，而处理的目标是让文件大小尽可能接近HDFS的block size大小或者整数倍。

- 明显小于block size文件的80%
- 129M:128M+1M

##### Hadoop小文件带来的问题

1. 众所周知，在HDFS中数据和元数据分别由DataNode和NameNode负责，这些元数据每个对象一般占用大约150个字节。大量的小文件相对于大文件会占用大量的NameNode内存。对NameNode内存管理产生巨大挑战，此外对JVM稳定性也有影响如GC。
2. 当NameNode重启时，它需要将文件系统元数据从本地磁盘加载到内存中。如果NameNode的元数据很大，重启速度会非常慢。
3. 一般来说，NameNode会不断跟踪并检查集群中每个block块的存储位置。这是通过DataNode的定时心跳上报其数据块来实现的。数据节点需要上报的block越多，则也会消耗越多的网络带宽/时延。
4. 更多的文件意味着更多的读取请求需要请求NameNode，这可能最终会堵塞NameNode的容量，增加RPC队列和处理延迟，进而导致性能和响应能力下降。
5. 对计算引擎如Spark、MapReduce性能造成负面影响。以MapReduce（以下简称MR）为例，大量小文件意味着大量的磁盘IO，磁盘IO通常是MR性能的最大瓶颈之一，在HDFS中对于相同数量的数据，一次大的顺序读取往往优于几次随机读取的性能。如果可以将数据存储在较少，而更大的一些block中，可以降低磁盘IO的性能影响。除了磁盘IO，还有内部任务的划分、资源分配等

##### Hadoop小文件是怎么来的

一个Hadoop集群中存在小文件的可能原因如下：

1. 流式任务（如spark streaming/flink等实时计算框架）在做数据处理时，无论是纯实时还是基于batch的准实时，在小的时间窗口内都可能产生大量的小文件。此外对于Spark任务如果过度并行化，每个分区一个文件，产生的文件也可能会增多
2. Hive分区表的过度分区这里的过度分区是指Hive分区表的每个分区数据量很小（比如小于HDFS block size）的Hive表。那么Hive Metastore Server调用开销会随着表拥有的分区数量而增加，影响性能。此时，要衡量数据量重新进行表结构设计（如减少分区粒度）。
3. 数据源有大量小文件，未做处理直接迁移到Hadoop集群。
4. 对于计算引擎处理任务，以MR为例。大量的map和reduce task存在。在HDFS上生成的文件基本上与map数量（对于Map-Only作业）或reduce数量（对于MR作业）成正比。此外，MR任务如果未设置合理的reduce数或者未做限制，每个reduce都会生成一个独立的文件。对于数据倾斜，导致大部分的数据都shuffle到一个或几个reduce，然后其他的reduce都会处理较小的数据量并输出小文件。对于Spark任务，过度并行化也是导致小文件过多的原因之一。在Spark作业中，根据写任务中提到的分区数量，每个分区会写一个新文件。这类似于MapReduce框架中的每个reduce任务都会创建一个新文件。Spark分区越多，写入的文件就越多。控制分区的数量来减少小文件的生成。

### 1.2.SQL On Hadoop

#### 1.2.1.SQL和存储

sql ==> 对应执行引擎的作业:MapReduce/Spark/Tez

Impala:内存
Presto:
Drill:
Phoenix:HBase(rowkey)
Spark SQL:

MetaStore:存储元数据,框架之间是共享元数据信息的

行式存储/列式存储

![C/R](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632376364(1).jpg)

#### 1.2.2.调优策略

1. 架构
   - 分表
   - 分区表(partition)
   - 中间结果集
   - 压缩
2. 语法
   - 排序:order by/partition by/distribute by/cluster by
   - 控制输出(reduce/partition/task)数量
   - join(普通join/map join)
   - 执行计划
3. 执行
   - 推测执行
   - 并行执行
   - JVM重用

##### 1. 架构

**分表:**

![分表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377124(1).jpg)

**分区表:**

![分区表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377381(1).jpg)

**中间结果集:**

![中间结果](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377645(1).jpg)

**压缩:**

![压缩](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377929(1).jpg)

![压缩详解](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632378025(1).jpg)

##### 2. 语法

**Order by / Sort by:**

Hive QL 中的\* ORDER BY 语法类似于 SQL 语言中的 ORDER BY \*语法。

---

colOrder: ( ASC | DESC )
colNullOrder: (NULLS FIRST | NULLS LAST)           -- (Note: Available in Hive 2.1.0 and later)
orderBy: ORDER BY colName colOrder? colNullOrder? (',' colName colOrder? colNullOrder?)*
query: SELECT expression (',' expression)* FROM src orderBy

---

order by子句中有一些限制。在严格模式下(即hive.mapred.mode = strict)，必须在 order by 子句后跟一个limit子句。如果将 hive.mapred.mode 设置为 nonstrict，则没有 limit 子句。原因是为了强加所有结果的总 Sequences，必须有一个减速器才能对最终输出进行排序。如果输出中的行数太大，则单个减速器可能需要很长时间才能完成。

对于 Hive 0.11.0 到 2.1.x，将hive.groupby.orderby.position.alias设置为 true(默认值为 false)。

对于 Hive 2.2.0 和更高版本，默认情况下hive.orderby.position.alias为 true。

SORT BY 语法类似于 SQL 语言中的 ORDER BY *语法。

---

colOrder: ( ASC | DESC )
sortBy: SORT BY colName colOrder? (',' colName colOrder?)*
query: SELECT expression (',' expression)* FROM src sortBy

---

Hive 使用\* SORT BY *中的列对行进行排序，然后再将行 Importing 到 reducer。排序 Sequences 将取决于列类型。如果列是数字类型，则排序 Sequences 也是数字 Sequences。如果列是字符串类型，则排序 Sequences 将是字典 Sequences。

在Hive 3.0.0及更高版本中，优化器将删除subqueries和views中的无限制排序。要禁用它，请将hive.remove.orderby.in.subquery设置为 false。

Hive 支持\* SORT BY *，可对每个 reducer 的数据进行排序。 “ order by”和“ sort by”之间的区别在于，前者保证输出中的总 Sequences，而后者仅保证精简器中行的排序。如果存在多个减速器，则“排序依据”可能会给出部分排序的最终结果。

注意：关于单个列的单独 SORT BY 与 CLUSTER BY 之间的区别可能会造成混淆。不同之处在于，如果有多个 reducer 分区，则 CLUSTER BY 按字段划分，而 SORT BY 则是随机划分，以便在 reducer 上均匀地分布数据(和负载)。

基本上，每个 reducer 中的数据将根据用户指定的 Sequences 进行排序。

**Distribute by/Cluster by:**

Cluster By 和 Distribute By *主要与Transform/Map-Reduce Scripts一起使用。但是，如果需要对查询的输出进行分区和排序以用于后续查询，有时在 SELECT 语句中很有用。

Cluster By 是 Distribute By 和 Sort By *的快捷方式。

Hive 使用\* Distribute By 中的列在reducer之间分配行。具有相同 Distribute By 列的所有行将进入相同的 reducer。但是， Distribute By *不保证分布式键上的聚类或排序属性。

---

例如，我们将以下 5 行的* x 分配给 2 个 reducer：

x1
x2
x4
x3
x1

==reducer 1==

x1
x2
x1

==reducer 2==

x4
x3
请注意，具有相同键 x1 的所有行都可以保证分配给同一reducer(在这种情况下为reducer 1)，但不能保证它们聚集在相邻位置。

相反，如果我们使用\* Cluster By x *，则两个化简器将进一步对 x 上的行进行排序：

==reducer 1==

x1
x1
x2

==reducer 2==

x3
x4
用户可以指定\* Distribute By 和 Sort By 来代替 Cluster By *，因此分区列和排序列可以不同。通常的情况是分区列是排序列的前缀，但这不是必需的。

```SQL
SELECT col1, col2 FROM t1 CLUSTER BY col1
SELECT col1, col2 FROM t1 DISTRIBUTE BY col1

SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC
FROM (
    FROM pv_users
    MAP ( pv_users.userid, pv_users.date )
    USING 'map_script'
    AS c1, c2, c3
    DISTRIBUTE BY c2
    SORT BY c2, c1) map_output
  INSERT OVERWRITE TABLE pv_users_reduced
    REDUCE ( map_output.c1, map_output.c2, map_output.c3 )
    USING 'reduce_script'
    AS date, count;
```

---

**总结:**

![区别](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632463246(1).jpg)

**执行计划(EXPLAIN):**

Hive 提供了一个EXPLAIN命令，该命令显示了查询的执行计划。该语句的语法如下：

EXPLAIN [EXTENDED|CBO|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE] query

HIVE 0.14.0 通过HIVE-5961支持AUTHORIZATION。 Hive 2.3.0 通过HIVE-11394支持VECTORIZATION。 Hive 3.2.0 通过HIVE-17683支持LOCKS。

AST 已从HIVE-13533中的 EXPLAIN EXTENDED 中删除，并在HIVE-15932中作为单独的命令恢复。

在EXPLAIN语句中使用EXTENDED会产生有关计划中运算符的更多信息。这通常是物理信息，例如文件名。

Hive 查询将转换为阶段序列(它更是有向非循环图)。这些阶段可以是 map/reduce 阶段，或者甚至可以是执行元存储或文件系统操作(如移动和重命名)的阶段。说明输出分为三个部分：

查询的抽象语法树

计划不同阶段之间的依赖性

每个阶段的描述

这些阶段本身的描述显示了一系列操作符，以及与操作符相关联的元数据。元数据可能包含诸如 FilterOperator 的过滤器表达式或 SelectOperator 的选择表达式或 FileSinkOperator 的输出文件名之类的内容。

**JOIN:**

LEFT SEMI JOIN 以有效的方式实现了不相关的 IN/EXISTS 子查询语义。从 Hive 0.13 开始，使用subqueries支持 IN/NOT IN/EXISTS/NOT EXISTS 运算符，因此这些 JOIN 中的大多数不再需要手动执行。使用 LEFT SEMI JOIN 的限制是只能在连接条件(ON 子句)中引用右侧表，而不能在 WHERE 或 SELECT 子句等中引用。

如果除一个要连接的表之外的所有表都很小，则可以将其作为仅 Map 作业执行。查询

```SQL

SELECT /*+ MAPJOIN(b) */ a.key, a.value
  FROM a JOIN b ON a.key = b.key

不需要reduce。对于 A 的每个 Map 器，B 都会被完全读取。
```

不支持以下内容。

union 后跟一个 MapJoin

横向视图后跟一个 MapJoin

减少接收器(分组依据/加入/排序依据/集群依据/分发依据)，其次是 MapJoin

MapJoin 之后是 union

MapJoin，然后加入

MapJoin 其次是 MapJoin

配置变量 hive.auto.convert.join(如果设置为 true)会在运行时自动将联接转换为 mapjoins，应使用它代替 mapjoin 提示。 mapjoin 提示仅应用于以下查询。

如果对所有 Importing 进行了存储分区或排序，则该联接应转换为存储分区的 Map 端连接或存储分区的排序合并联接。

考虑在不同的键上使用多个 mapjoin 的可能性：

```SQL
select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM
  ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM
    bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)
  ) firstjoin
  JOIN
  smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo)
```

不支持上述查询。如果没有 mapjoin 提示，则以上查询将作为 2 个仅 map 作业执行。如果用户事先知道 Importing 足够小以适合内存，则可以使用以下可配置参数来确保查询在单个 map-reduce 作业中执行。

hive.auto.convert.join.noconditionaltask-Hive 是否启用基于 Importing 文件大小的关于将普通联接转换为 mapjoin 的优化。如果启用此参数，并且 n 向联接的表/分区的 n-1 个大小的总和小于指定的大小，则该联接将直接转换为 mapjoin(没有条件任务)。

hive.auto.convert.join.noconditionaltask.size-如果关闭了 hive.auto.convert.join.noconditionaltask，则此参数不起作用。但是，如果启用了该连接，并且 n 向联接的表/分区的 n-1 个大小的总和小于此大小，则该联接将直接转换为 mapjoin(没有条件任务)。默认值为 10MB。

Hive 会自动识别各种用例并对其进行优化。 Hive 0.11 改进了以下情况的优化程序：

在一侧适合内存的位置连接。在新的优化中：

将该那一侧作为哈希表加载到内存中

只需要扫描较大的表

事实表在内存中的占用空间较小

Star-schema joins

在许多情况下，不再需要提示。

Map 联接由优化器自动拾取。

![JOIN](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632465053(1).jpg)

##### 3. 运行层面的调优

**推测执行:**

hive.mapred.reduce.tasks.speculative.execution
默认值：true

添加于：Hive0.5.0

是否应该对减速器进行投机执行。

![推测执行](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150517.bmp)

**并行执行:**

hive.exec.parallel
默认值：false

添加于：Hive0.5.0

是否并行执行作业。适用于可以并行运行的 MapReduce 作业，例如，在连接之前处理不同源表的作业。从Hive 0.14开始，也适用于可并行运行的移动任务，例如，在多插入期间移动文件以插入目标。

hive.exec.parallel.thread.number
默认值：8

添加于：Hive0.6.0

最多可以并行执行多少个作业。

![并行执行](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150835.bmp)

**JVM重用:**

Hadoop与JVM重用对应的参数是mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM。

比如在集群中配置每个slave节点最多同时运行16个map和2个reduce。那slave节点会启动最多16个JVM用于map。

为每个task启动一个新的JVM将耗时1秒左右，对于运行时间较长（比如1分钟以上）的job影响不大，但如果都是时间很短的task，那么频繁启停JVM会有开销。

如果我们想使用JVM重用技术来提高性能，那么可以将mapred.job.reuse.jvm.num.tasks设置成大于1的数。这表示属于同一job的顺序执行的task可以共享一个JVM，也就是说第二轮的map可以重用前一轮的JVM，而不是第一轮结束后关闭JVM，第二轮再启动新的JVM。

那么最多一个JVM能顺序执行多少个task才关闭呢？这个值就是mapred.job.reuse.jvm.num.tasks。如果设置成-1，那么只要是同一个job的task（无所谓多少个），都可以按顺序在一个JVM上连续执行。

如果task属于不同的job，那么JVM重用机制无效，不同job的task需要不同的JVM来运行。

JVM重用技术不是指同一Job的两个或两个以上的task可以同时运行于同一JVM上，而是排队按顺序执行。

一个tasktracker最多可以同时运行的task数目由mapred.tasktracker.map.tasks.maximum和mapred.tasktracker.reduce.tasks.maximum

决定，并且这两个参数在mapred-site.xml中设置。其他方法，如在JobClient端通过命令行-Dmapred.tasktracker.map.tasks.maximum=number或者conf.set("mapred.tasktracker.map.tasks.maximum","number")设置都是无效的。

![jvm优化](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150858.bmp)

## 2.Spark

### 2.1.算子

#### 2.1.1.map & mapPartition

分析一下相关的源码，通过这种方式，来分析，二者之间究竟有什么异同。RDD部分代码如下：

```scala
    def map(f: T => U): RDD[U] = withScope {
      val cleanF = sc.clean(f)
      new MapPartitionsRDD(this, (..., iter) => iter.map(cleanF))
    }
```

```scala
    def mapPartitions(f: Iterator[T] => Iterator[U], ...): RDD[U] = withScope {
      val cleanedF = sc.clean(f)
      new MapPartitionsRDD(
        this,
        (..., iter: Iterator[T]) => cleanedF(iter),
        ...)
    }
```

上面的代码比较简单，我们来分析一下：

```scala
可以看到，其实二者返回的都是一个MapPartitionsRDD的实例，传入的第二个参数f有所不同，

而这个f是在RDD的compute中调用的（具体代码后面给出），也就是说这个f是二者的根本区别

我们关注MapPartitionsRDD构造函数的第二个参数：

f: (TaskContext, Int, Iterator) => Iterator map

最后的实参是f = (..., iter) =>iter map cleanedFmapPartitions 的是

f = (..., iter) => cleanedF(iter) cleaned直接当成我们传进来的函数就好，怎么来的这里不关注。

刚才说到，f是compute调用的，而compute是在节点上运行task的时候间接触发的

所以，最后其实就是在单机上，对iter的不同操作方式的区别！

下面我们来想一下，iter map f与f(iter)有什么异同？

前者，会遍历整个iter并且返回相同size的新的迭代器，

而后者可以根据自定义f来操作iter，结果的size可能会发生变化

当然也可能不需遍历整个iter ，比如可能我想返回这个

Iterator( iter.head )如果需要诸如connection的创建

哪个性能高，一下便知二者都不会立即计算，都只能通过

最后的writer.write(rdd.iterator)来触发真正的计算 

没有条件2的情况下，通常性能差别并不大，也通常不会成为瓶颈，没有想象的那么严重

```

![map&mapPartition](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926142704.bmp)

#### 2.1.2.foreach & foreachPartition

基于socket word count写入外部存储mysql ，记录下他们的区别。

```scala

package com.imooc.spark.streaming

import java.sql.DriverManager

import com.imooc.spark.util.ConnectionPool
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * 使用Spark Streaming完成词频统计，并将结果写入到MySQL数据库中
  */
object ForeachRDDApp {

  def main(args: Array[String]): Unit = {

    val sparkConf = new SparkConf().setAppName("ForeachRDDApp").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(5))


    val lines = ssc.socketTextStream("localhost", 6789)

    val result = lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)

    //result.print()  //此处仅仅是将统计结果输出到控制台

    result.foreachRDD(rdd => {
      rdd.foreachPartition(partitionOfRecords => {
        val connection = ConnectionPool.getConnection()
        partitionOfRecords.foreach(record => {
          val sql = "insert into streaming(item, count) values('" + record._1 + "'," + record._2 + ")"
          connection.createStatement().execute(sql)
        })

        ConnectionPool.returnConnection(connection)
      })
    })


    ssc.start()
    ssc.awaitTermination()
  }
}

```

```scala

foreachRDD 源码：


  /**
   * Apply a function to each RDD in this DStream. This is an output operator, so
   * 'this' DStream will be registered as an output stream and therefore materialized.
   */
  def foreachRDD(foreachFunc: RDD[T] => Unit): Unit = ssc.withScope {
    val cleanedF = context.sparkContext.clean(foreachFunc, false)
    foreachRDD((r: RDD[T], _: Time) => cleanedF(r), displayInnerRDDOps = true)
  }

最常用的输出算子，func作用在输出流数据生成的每个RDD,这个函数应该可以把每个RDD中的数据输出到外部系统，
比如存储rdd到文件，通过网络存储到数据库, func 是在运行streming application的driver端进程
注意只有actions算子才会触发RDD的计算。DStream中即使有foreachRDD算子也不会即使进行处理，
只有foreachRDD(func)函数func中存在了action算子才会执行运算，
所以foreachRDD的函数中可以使用foreach和foreachPartition算子来触发action操作。



foreachPartition源码：

/**
 * Applies a function f to each partition of this RDD.
 */
def foreachPartition(f: Iterator[T] => Unit): Unit = withScope {
  val cleanF = sc.clean(f)
  sc.runJob(this, (iter: Iterator[T]) => cleanF(iter))
}

Spark core中的算子
foreachPartition是对每个partition中的iterator实行迭代的处理.
通过用户传入的function（即函数f）对iterator进行内容的处理，
源码中函数f传入的参数是一个迭代器，也就是说在foreachPartiton中函数处理的是分区迭代器，
而非具体的数据


foreach 源码：

/** Applies a function `f` to all values produced by this iterator.
 *
 *  @param  f   the function that is applied for its side-effect to every element.
 *              The result of function `f` is discarded.
 *
 *  @tparam  U  the type parameter describing the result of function `f`.
 *              This result will always be ignored. Typically `U` is `Unit`,
 *              but this is not necessary.
 *
 *  @note    Reuse: $consumesIterator
 *
 *  @usecase def foreach(f: A => Unit): Unit
 *    @inheritdoc
 */
def foreach[U](f: A => U) { while (hasNext) f(next()) }

与foreachPartition类似的是，foreach也是对每个partition中的iterator中的所有值实行迭代处理，
通过用户传入的function（即函数f）对iterator进行内容的处理。
而不同的是，函数f中的参数传入的不再是一个迭代器，而是每次的foreach得到的一个rdd的kv实例，
也就是具体的数据，例如更新一个累加器或与一个外部存储系统交互比如本例中的执行insert操作。
```

1. foreachRDD 是spark streaming 的最常用的output 算子，foreachPartition和foreach 是spark core的算子
2. foreachRDD是执行在driver端，其他两个是执行在exectuor端
3. foreachRDD 输入rdd, 其他两个传入的是iterator, foreachPartition传入的迭代器，foreach传入的是迭代器产生的所有值进行处理，举例说明foreachpartion是每个分区执行一遍，比如说m个分区，n个数据，foreachpartion会执行m次，foreach会执行m*n次

![foreach & foreachPartition](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926144822.png)

#### 2.1.3.reduceByKey & groupByKey

reduceByKey按照key进行聚合，在shuffle之前有预聚合（combine）操作，返回结果为RDD[k,v]。2. groupByKey按照key进行分组后直接shuffle（无预聚合）。预聚合可以提高执行性能，在不影响业务逻辑的情况下，建议优先使用reduceByKey。

- groupByKey 不会在map端进行combine，而reduceByKey 会在map端的默认开启combine进行本地聚合。
- 在map端先进行一次聚合，很极大的减小reduce端的压力，一般来说，map的机器数量是远大于reduce的机器数量的。通过map聚合的方式可以把计算压力平均到各台机器，最终在reduce端只需要汇总map端聚合完成的数据即可，极大的提高效率。

![reduce/group ByKey code](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926151524.png)

![reduce/group ByKey](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926151128.png)

#### 2.1.4.collect

数组 Array 的形式返回数据集的所有元素，具体为将不同分区的数据按照分区顺序采集到Driver端内存中，形成数组。

在驱动程序中，以数组Array的形式返回数据集的所有元素

这个方法可以将RDD类型的数据转化为数组，同时会从远程集群拉取数据到driver端（组成一维数组）。

```scala
import org.apache.spark.{SparkConf, SparkContext}

object action {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    val rdd = sc.makeRDD(List(1,2,3,4),2)
    val ints = rdd.collect()
    println(ints.mkString(","))

    sc.stop()
  }

}

结果：
1,2,3,4
```

![collect](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926152425.png)

#### 2.1.5.coalesce & repartiton

**coalesce**
函数功能：改变原始数据的分区，减少分区数量。coalesce方法默认情况下不会将分区的数据打乱重新组合

有俩个参数：

- numPartitions:（Int） ：设置分区数
- shuffle:（Boolean ）：为Ture时，会进行suffle操作，将之前的分区重新分配，为false时，则不会进行shuffle操作

作用：大量数据进行过滤后，将数据量的小的分区，重新划分为一个分区，提高处理效率。

```scala
package com.atguigu.bigdata.spark.core.RDD.operator.transform

import org.apache.spark.{SparkConf, SparkContext}

object coalesce {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("operator")
    val sc = new SparkContext(sparkConf)
    //filter
    val rdd  = sc.makeRDD(List(1,2,3,4,5,6), 3)
    val newRDD = rdd.coalesce(2)

    newRDD.saveAsTextFile("output")
    sc.stop()

  }

}

实例2:

package com.atguigu.bigdata.spark.core.rdd.operator.transform

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Spark10_RDD_Operator_Transform {

    def main(args: Array[String]): Unit = {

        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
        val sc = new SparkContext(sparkConf)

        // TODO 算子 - filter
        val rdd = sc.makeRDD(List(1,2,3,4,5,6), 3)

        // coalesce方法默认情况下不会将分区的数据打乱重新组合
        // 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜
        // 如果想要让数据均衡，可以进行shuffle处理
        //val newRDD: RDD[Int] = rdd.coalesce(2)
        val newRDD: RDD[Int] = rdd.coalesce(2,true)

        newRDD.saveAsTextFile("output")
        sc.stop()

    }
}
```

**repartiion**
repartition用来调整父RDD的分区数，入参为调整之后的分区数。由于使用方法比较简单，这里就不写例子了。

```scala
  def repartition(numPartitions: Int): DStream[T] = ssc.withScope {
    this.transform(_.repartition(numPartitions))
  }
```

从方法中可以看到，实现repartition的方式是通过Dstream的transform算子之间调用RDD的repartition算子实现的。

接下来就是看看RDD的repartition算子是如何实现的。

```scala
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }

```

首先可以看到RDD的repartition的实现是调用时coalesce方法。其中入参有两个第一个是numPartitions为重新分区后的分区数量，第二个参数为是否shuffle，这里的入参为true代表会进行shuffle。

他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）

1）N<M。一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。

2）如果N>M并且N和M相差不多， (假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false， 在shuffl为false的情况下，如果M>N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。

3）如果N>M并且两者相差悬殊，这时如果将shuffle设置为false，父子RDD是窄依赖关系，他们同处在一个Stage中，就可能造成Spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。

总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDDde分区数变多的。

#### 2.1.6.cache & persist

Spark 中一个很重要的能力是将数据持久化（或称为缓存），在多个操作间都可以访问这些持久化的数据。

当持久化一个 RDD 时，每个节点的其它分区都可以使用 RDD 在内存中进行计算，在该数据上的其他 action 操作将直接使用内存中的数据。

这样会让以后的 action 操作计算速度加快（通常运行速度会加速 10 倍）。

缓存是迭代算法和快速的交互式使用的重要工具。

RDD 可以使用 persist() 方法或 cache() 方法进行持久化。

![cache和persist](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926154947.png)

数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。

在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 persist 方法，Spark 也会自动缓存部分中间数据。

这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法。

每个持久化的 RDD 可以使用不同的存储级别进行缓存

例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。

这些存储级别通过传递一个 StorageLevel 对象给 persist() 方法进行设置。

详细的存储级别介绍如下：

- MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。
- MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。
- MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。
- MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。DISK_ONLY : 只在磁盘上缓存 RDD。
- MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。
- OFF_HEAP）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。

**选择存储级别**
Spark 的存储级别的选择，核心问题是在内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择 :

- 如果使用默认的存储级别（MEMORY_ONLY），存储在内存中的 RDD 没有发生溢出，那么就选择默认的存储级别。默认存储级别可以最大程度的提高 CPU 的效率,可以使在 RDD 上的操作以最快的速度运行。
- 如果内存不能全部存储 RDD，那么使用 MEMORY_ONLY_SER，并挑选一个快速序列化库将对象序列化，以节省内存空间。使用这种存储级别，计算速度仍然很快。
- 除了在计算该数据集的代价特别高，或者在需要过滤大量数据的情况下，尽量不要将溢出的数据存储到磁盘。因为，重新计算这个数据分区的耗时与从磁盘读取这些数据的耗时差不多。
- 如果想快速还原故障，建议使用多副本存储级别（例如，使用 Spark 作为 web 应用的后台服务，在服务出故障时需要快速恢复的场景下）。所有的存储级别都通过重新计算丢失的数据的方式，提供了完全容错机制。但是多副本级别在发生数据丢失时，不需要重新计算对应的数据库，可以让任务继续运行。

### 2.2.序列化

#### 2.2.1.序列化之java序列化

Java序列化: 默认情况下,使用Java的 Spark 序列化对象 ObjectOutputStream 框架,你可以使用任何实现了 java.io.Serializable 的类与它一起使用。你还可以通过继承 java.io.Externalizable 来更深入地控制序列化的性能。Java序列化有着灵活，但通常非常缓慢的特性，会导致许多类的大型序列化格式。

![java序列化操作](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926160954.png)

#### 2.2.2.序列化之Kryo序列化

Kryo serialization: Spark还可以使用Kryo库(version 4)更快地序列化对象。Kryo比Java序列化要快得多(通常多达10倍)，也更紧凑，但它不支持所有的 Serializable 类型，并且要求你预先 注册 将在程序中使用的类，以获得最佳性能。

你可以通过使用 SparkConf 初始化作业并调用 conf.set("spark.serializer", org.apache.spark.serializer.KryoSerializer")，切换使用 Kryo 来进行序列化。此设置配置的序列化程序不仅用于在工作节点之间数据 shuffle，而且还用于将 RDD 序列化到磁盘。Kryo不是默认的 serializer 的唯一原因是自定义的注册要求，但是我们建议在任何网络密集型应用程序中尝试它。自从 Spark 2.0.0 以来，我们在使用简单类型、简单类型数组或字符串类型 RDD 之间 shuffle 时，在内部使用 Kryo serializer。

Spark 默认包含 Kryo 序列化器，用于 Twitter chill 库中 AllScalaRegistrar 涉及的许多常用的Scala核心类。

要向Kryo注册你自己的自定义类，请使用 registerKryoClasses 方法。

```scala

  val conf = new SparkConf().setMaster(...).setAppName(...)
  conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))
  val sc = new SparkContext(conf)

```

如果你的对象很大，你可能还需要增加 spark.kryoserializer.buffer config。这个值需要足够大，以容纳你要序列化的最大对象。

最后，如果你不注册自定义类，Kryo仍然可以工作，但是它必须将完整的类名存储在每个对象中，这是很浪费资源的。

![Kryo](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926161435.png)

![java vs kryo](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926161504.png)

### 2.3.流数据处理

#### 2.3.1.Sink数据到MySQL

**foreachRDD(func):**

对从流中生成的每个 RDD 应用函数 func 的最通用的输出运算符。此功能应将每个 RDD 中的数据推送到外部系统，例如将 RDD 保存到文件，或将其通过网络写入数据库。请注意，函数 func 在运行流应用程序的 driver 进程中执行，通常会在其中具有 RDD 动作，这将强制流式传输 RDD 的计算。

dstream.foreachRDD 是一个强大的原语，允许将数据发送到外部系统。但是，了解如何正确有效地使用这个原语很重要。避免一些常见的错误如下。

通常向外部系统写入数据需要创建连接对象（例如与远程服务器的 TCP 连接），并使用它将数据发送到远程系统。为此，开发人员可能会无意中尝试在Spark driver 中创建连接对象，然后尝试在Spark工作人员中使用它来在RDD中保存记录。例如（在 Scala 中）:

```scala

dstream.foreachRDD { rdd =>
  val connection = createNewConnection()  // executed at the driver
  rdd.foreach { record =>
    connection.send(record) // executed at the worker
  }
}

dstream.foreachRDD(rdd -> {
  Connection connection = createNewConnection(); // executed at the driver
  rdd.foreach(record -> {
    connection.send(record); // executed at the worker
  });
});

def sendRecord(rdd):
    connection = createNewConnection()  # executed at the driver
    rdd.foreach(lambda record: connection.send(record))
    connection.close()

dstream.foreachRDD(sendRecord)

```

这是不正确的，因为这需要将连接对象序列化并从 driver 发送到 worker。这种连接对象很少能跨机器转移。此错误可能会显示为序列化错误（连接对象不可序列化），初始化错误（连接对象需要在 worker 初始化）等。正确的解决方案是在 worker 创建连接对象。

但是，这可能会导致另一个常见的错误 - 为每个记录创建一个新的连接。例如:

```scala

dstream.foreachRDD { rdd =>
  rdd.foreach { record =>
    val connection = createNewConnection()
    connection.send(record)
    connection.close()
  }
}

dstream.foreachRDD(rdd -> {
  rdd.foreach(record -> {
    Connection connection = createNewConnection();
    connection.send(record);
    connection.close();
  });
});

def sendRecord(record):
    connection = createNewConnection()
    connection.send(record)
    connection.close()

dstream.foreachRDD(lambda rdd: rdd.foreach(sendRecord))

```

通常，创建连接对象具有时间和资源开销。因此，创建和销毁每个记录的连接对象可能会引起不必要的高开销，并可显着降低系统的总体吞吐量。一个更好的解决方案是使用 rdd.foreachPartition - 创建一个连接对象，并使用该连接在 RDD 分区中发送所有记录。

```scala

dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    val connection = createNewConnection()
    partitionOfRecords.foreach(record => connection.send(record))
    connection.close()
  }
}

dstream.foreachRDD(rdd -> {
  rdd.foreachPartition(partitionOfRecords -> {
    Connection connection = createNewConnection();
    while (partitionOfRecords.hasNext()) {
      connection.send(partitionOfRecords.next());
    }
    connection.close();
  });
});

def sendPartition(iter):
    connection = createNewConnection()
    for record in iter:
        connection.send(record)
    connection.close()

dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))

```

这样可以在多个记录上分摊连接创建开销。

最后，可以通过跨多个RDD /批次重用连接对象来进一步优化。可以维护连接对象的静态池，而不是将多个批次的 RDD 推送到外部系统时重新使用，从而进一步减少开销。

```scala

dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    // ConnectionPool is a static, lazily initialized pool of connections
    val connection = ConnectionPool.getConnection()
    partitionOfRecords.foreach(record => connection.send(record))
    ConnectionPool.returnConnection(connection)  // return to the pool for future reuse
  }
}

dstream.foreachRDD(rdd -> {
  rdd.foreachPartition(partitionOfRecords -> {
    // ConnectionPool is a static, lazily initialized pool of connections
    Connection connection = ConnectionPool.getConnection();
    while (partitionOfRecords.hasNext()) {
      connection.send(partitionOfRecords.next());
    }
    ConnectionPool.returnConnection(connection); // return to the pool for future reuse
  });
});

def sendPartition(iter):
    # ConnectionPool is a static, lazily initialized pool of connections
    connection = ConnectionPool.getConnection()
    for record in iter:
        connection.send(record)
    # return to the pool for future reuse
    ConnectionPool.returnConnection(connection)

dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))

```

请注意，池中的连接应根据需要懒惰创建，如果不使用一段时间，则会超时。这实现了最有效地将数据发送到外部系统.

其他要记住的要点:
DStreams 通过输出操作进行延迟执行，就像 RDD 由 RDD 操作懒惰地执行。具体来说，DStream 输出操作中的 RDD 动作强制处理接收到的数据。因此，如果您的应用程序没有任何输出操作，或者具有 dstream.foreachRDD() 等输出操作，而在其中没有任何 RDD 操作，则不会执行任何操作。系统将简单地接收数据并将其丢弃。

默认情况下，输出操作是 one-at-a-time 执行的。它们按照它们在应用程序中定义的顺序执行。

#### 2.3.2.Sprak Steraming 整合 Kafka

Spark streaming接收Kafka数据用spark streaming流式处理kafka中的数据，第一步当然是先把数据接收过来，转换为spark streaming中的数据结构Dstream。接收数据的方式有两种：

1. 利用Receiver接收数据
2. 直接从kafka读取数据

**接收器（Receiver）:**

这种方式利用接收器（Receiver）来接收kafka中的数据，其最基本是使用Kafka高阶用户API接口。对于所有的接收器，从kafka接收来的数据会存储在spark的executor中，之后spark streaming提交的job会处理这些数据。

在使用时，我们需要添加相应的依赖包：

```xml
<dependency><!-- Spark Streaming Kafka -->
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka_2.10</artifactId>
    <version>1.6.3</version>
</dependency>
```

而对于Scala的基本使用方式如下：

```scala
import org.apache.spark.streaming.kafka._

 val kafkaStream = KafkaUtils.createStream(streamingContext, 
     [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])

```

注意的点：

- 在Receiver的方式中，Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度。
- 对于不同的Group和topic我们可以使用多个Receiver创建不同的Dstream来并行接收数据，之后可以利用union来统一成一个Dstream。
- 如果我们启用了Write Ahead Logs复制到文件系统如HDFS，那么storage level需要设置成 StorageLevel.MEMORY_AND_DISK_SER，也就是KafkaUtils.createStream(..., StorageLevel.MEMORY_AND_DISK_SER)

**直接读取方式:**

在spark1.3之后，引入了Direct方式。不同于Receiver的方式，Direct方式没有receiver这一层，其会周期性的获取Kafka中每个topic的每个partition中的最新offsets，之后根据设定的maxRatePerPartition来处理每个batch。

这种方法相较于Receiver方式的优势在于：

- 简化的并行：在Receiver的方式中我们提到创建多个Receiver之后利用union来合并成一个Dstream的方式提高数据传输并行度。而在Direct方式中，Kafka中的partition与RDD中的partition是一一对应的并行读取Kafka数据，这种映射关系也更利于理解和优化。
- 高效：在Receiver的方式中，为了达到0数据丢失需要将数据存入Write Ahead Log中，这样在Kafka和日志中就保存了两份数据，浪费！而第二种方式不存在这个问题，只要我们Kafka的数据保留时间足够长，我们都能够从Kafka进行数据恢复。
- 精确一次：在Receiver的方式中，使用的是Kafka的高阶API接口从Zookeeper中获取offset值，这也是传统的从Kafka中读取数据的方式，但由于Spark Streaming消费的数据和Zookeeper中记录的offset不同步，这种方式偶尔会造成数据重复消费。而第二种方式，直接使用了简单的低阶Kafka API，Offsets则利用Spark Streaming的checkpoints进行记录，消除了这种不一致性。

#### 2.3.3.Spark向kafka中写入数据

首先，我们需要将KafkaProducer利用lazy val的方式进行包装如下：

```scala
import java.util.concurrent.Future
import org.apache.kafka.clients.producer.{ KafkaProducer, ProducerRecord, RecordMetadata }
class KafkaSink[K, V](createProducer: () => KafkaProducer[K, V]) extends Serializable {
  /* This is the key idea that allows us to work around running into
     NotSerializableExceptions. */
  lazy val producer = createProducer()
  def send(topic: String, key: K, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, key, value))
  def send(topic: String, value: V): Future[RecordMetadata] =
    producer.send(new ProducerRecord[K, V](topic, value))
}

object KafkaSink {
  import scala.collection.JavaConversions._
  def apply[K, V](config: Map[String, Object]): KafkaSink[K, V] = {
    val createProducerFunc = () => {
      val producer = new KafkaProducer[K, V](config)
      sys.addShutdownHook {
        // Ensure that, on executor JVM shutdown, the Kafka producer sends
        // any buffered messages to Kafka before shutting down.
        producer.close()
      }
      producer
    }
    new KafkaSink(createProducerFunc)
  }
  def apply[K, V](config: java.util.Properties): KafkaSink[K, V] = apply(config.toMap)
}

之后我们利用广播变量的形式，将KafkaProducer广播到每一个executor，如下：
// 广播KafkaSink
val kafkaProducer: Broadcast[KafkaSink[String, String]] = {
  val kafkaProducerConfig = {
    val p = new Properties()
    p.setProperty("bootstrap.servers", Conf.brokers)
    p.setProperty("key.serializer", classOf[StringSerializer].getName)
    p.setProperty("value.serializer", classOf[StringSerializer].getName)
    p
  }
  log.warn("kafka producer init done!")
  ssc.sparkContext.broadcast(KafkaSink[String, String](kafkaProducerConfig))
}

这样我们就能在每个executor中愉快的将数据输入到kafka当中：

//输出到kafka
segmentedStream.foreachRDD(rdd => {
  if (!rdd.isEmpty) {
    rdd.foreach(record => {
      kafkaProducer.value.send(Conf.outTopics, record._1.toString, record._2)
      // do something else
    })
  }
})

```

### 2.4.Spark问题

#### 2.4.1.Spark On Yarn

有两种部署模式可以用于在 YARN 上启动 Spark 应用程序。在 cluster 集群模式下，Spark driver 运行在集群上由 YARN 管理的application master 进程内，并且客户端可以在初始化应用程序后离开。在 client 客户端模式下，driver 在客户端进程中运行，并且 application master 仅用于从 YARN 请求资源。

Spark On Yarn 有两种运行模式:

- Yarn - Cluster
- Yarn - Client

他们的主要区别是:

- Cluster: Spark的Driver在App Master主进程内运行, 该进程由集群上的YARN管理, 客户端可以在启动App Master后退出.
- Client: Driver在提交作业的Client中运行, App Master仅用于从YARN请求资源.

**在 cluster 集群模式下启动 Spark 应用程序:**

```shell

$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options] 

例如:

$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    lib/spark-examples*.jar \
    10

```

上面启动一个 YARN 客户端程序，启动默认的 Application Master。然后 SparkPi 将作为 Application Master 的子进程运行。客户端将定期轮询 Application Master 以获取状态的更新并在控制台中显示它们。一旦您的应用程序完成运行后，客户端将退出。请参阅下面的 “调试应用程序” 部分，了解如何查看 driver 和 executor 的日志。

**在 client 模式下启动 Spark 应用程序:**

要在 client 客户端模式下启动 Spark 应用程序，请执行相同的操作，但是将 cluster 替换 client。下面展示了如何在 client 客户端模式下运行 spark-shell:

```shell

./bin/spark-shell --master yarn --deploy-mode client

```

添加其他的 JARs
在 cluster 集群模式下，driver 在与客户端不同的机器上运行，因此 SparkContext.addJar 将不会立即使用客户端本地的文件运行。要使客户端上的文件可用于 SparkContext.addJar，请在启动命令中使用 --jars 选项来包含这些文件。

```shell

$ ./bin/spark-submit --class my.main.Class \
    --master yarn \
    --deploy-mode cluster \
    --jars my-other-jar.jar,my-other-other-jar.jar \
    my-main-jar.jar \
    app_arg1 app_arg2

```

#### 2.4.2.Spark内存管理

**内存调优**
在内存使用时进行调优，有三个考虑因素:

- 内存大小： 对象使用的内存 大小 (你可能希望整个 dataset 都能在内存中)
- 成本： 访问对象的成本
- 开销： gc 开销(如果对象的周转率很高)。

默认情况下，Java对象的访问速度很快，但是很容易比字段中的 “原始(raw)” 数据多消耗2-5倍的内存空间。这是由于以下几个原因造成的:

每个不同的 Java 对象都有一个"对象头"，它大约有16个字节，包含指向其类的指针等信息。对于一个只有很少数据的对象(比如一个 Int 字段)，它可比数据大。

Java String 在原始字符串数据上大约有40个字节的开销(因为他们将其存储在 Char 数组中，并保留额外的数据，如长度)，由于 String 内部使用 UTF-16 编码，因此将每个字符存储为 *2字节*。因此，一个10个字符的字符串可以轻松地消耗60字节空间。

常见的集合类，如 HashMap 和 LinkedList，使用链接数据结构，其中每个元素都有一个 "wrapper" 对象(例如“Map.Entry”)。这个对象不仅有一个头，而且还有指向列表中下一个对象的地址指针(通常每个地址指针有 8 个字节)。

原始类型的集合通常将它们存储为 boxed (装箱) 对象，如 java.lang.Integer
本节将首先概述Spark中的内存管理，然后讨论用户可以采取的具体策略，以便在他/她的应用程序中更有效地使用内存。

**内存管理概论**
Spark中的内存使用主要分为两类: 执行和存储。

- 执行内存: 是指用于 shuffle 、join、sort 和 aggregation 的计算
- 存储内存: 是指用于在集群中 cache 和传播内部数据的内存。

在Spark中，执行和存储共享一个统一的区域(M)，当没有执行内存时，存储可以获取所有可用的内存，反之亦然。如果有必要，执行可能会驱逐存储，但仅在总存储内存使用量低于某个阈值(R)时才会这样做。换句话说，R 描述了 M 中的一个子区域，其中缓存的块永远不会被驱逐。存储可能不会因为执行的复杂性而驱逐执行。

这种设计确保了几个理想的性能。首先，不使用缓存的应用程序可以将整个空间用于执行，从而避免不必要的磁盘溢出。其次，使用缓存的应用程序可以保留最小的存储空间(R)，在这里它们的数据块不会被清除。最后，这种方法为各种工作负载提供了合理的开箱即用性能，而不需要用户了解如何在内部划分内存。

虽然有两种相关的配置，但是典型的用户不应该需要调整它们，因为默认值适用于大多数工作负载:

spark.memory.fraction 表示 M 的大小是(JVM heap space- 300MB)的一部分(默认值0.6)。剩下的空间(40%)用于用户数据结构、Spark中的内部元数据，以及在稀疏和异常大的记录情况下防止 OOM 错误。

spark.memory.storageFraction 将 R 的大小表示为 M 的一部分(默认值为0.5)。R 是 M 中的存储空间，缓存的块不会被执行清除。
应该设置 spark.memory.fraction 的值，以便在JVM的 老年代(old)或 永久代(tenured)中合适地容纳这部分堆空间。

**确定内存消耗**
确定一个 dataset 所需的内存消耗的最好方法是创建一个RDD，将它放到缓存中，然后在web UI中查看 “Storage” 页面。该页将告诉你RDD占用了多少内存。

要估算特定对象的内存消耗，请使用 SizeEstimator 的 estimate 方法。这对于尝试使用不同的数据布局来调整内存使用，以及确定广播变量在每个执行器堆上占用的空间量非常有用。

**优化数据结构**
减少内存消耗的第一种方法是避免使用增加开销的Java特性，比如 pointer-based 的数据结构和 wrapper 对象。

有几种方法可以做到这一点:

设计你自己的数据结构以选择对象数组和基本类型，而不是标准的 Java 或 Scala 集合类(e.g. HashMap)。fastutil 库为与 Java标准 library 兼容的基本类型提供了方便的集合类。

尽可能避免嵌套有大量小对象和指针的结构。

考虑使用数字id或枚举对象代替 key 的字符串。

如果你的内存不足32 GB，那么可以设置 JVM参数 -XX:+UseCompressedOops，使地址指针变成4个字节，而不是8个字节。你可以在 spark-env.sh 中添加这些选项。

**序列化 RDD 存储**
当对象仍过于庞大,尽管进行了上述调优，仍无法有效地进行存储。 减少内存使用一个更简单的方法是将它们用 serialized 形式进行存储。 在 RDD 持久 API 使用序列化 StorageLevels, 如 MEMORY_ONLY_SER。Spark 然后将每个RDD分区存储为一个大字节数组。以序列化形式存储数据的惟一缺点是访问时间较慢，因为必须动态地反序列化每个对象。如果你希望以序列化的形式缓存数据，我们强烈建议使用Kryo，因为它比Java序列化(当然也比原始Java对象)的大小小得多。

**GC优化**
当你的程序存储了大量的 RDD 数据时， JVM GC 可能会成为一个问题。(对于那些只读取一次RDD，然后在上面运行许多操作的程序来说，这通常不是问题。)

当Java需要清除旧对象来为新对象腾出空间时，它将需要跟踪所有Java对象并找到未使用的对象。这里要记住的要点是，GC成本与Java对象的数量*成比例，因此使用对象较少的数据结构(例如，一个 Int 数组而不是 LinkedList数组)可以大大降低这一成本。一个更好的方法是以序列化的形式持久化对象，如上所述:现在每个RDD分区将只有 一个 对象(一个字节数组)。在尝试其他技术之前，如果GC有问题，首先要尝试的是使用[序列化缓存](#序列化 RDD 存储)。

GC也可能是一个问题，因为任务的工作内存(运行任务所需的空间量)和节点上缓存的 RDD 之间存在干扰。我们将讨论如何控制分配给RDD缓存的空间来缓解这种情况。

**测量GC的影响**
GC调优的第一步是收集关于 GC发生的频率和GC花费的时间的统计信息。

这可以通过在Java 参数中添加 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps 来实现。(请参阅配置指南以获取关于将Java参数传递给Spark作业的信息。)

下一次运行Spark作业时，你将看到每次发生垃圾收集时在 Worker 日志中打印的消息。注意，这些日志将在你的集群Worker 节点 上(在它们的工作目录中的 stdout 文件中)，而不是在你的 Driver 进程上。

**高级GC调优**
为了进一步调优 GC，我们首先需要了解JVM中关于内存管理的一些基本信息:

Java堆空间分为两个区域，年轻代(Young) 和老年代(Old) 。年轻代的目的是持有生命周期较短（short-object）的对象，而老年代的目的是持有具有更长的生存期的对象。
年轻一代被进一步划分为三个区域[Eden, Survivor1, Survivor2]。

GC 过程的简化描述:
当Eden已满时，在Eden上运行一个小型GC，从Eden和Survivor1存活的对象被复制到Survivor2。交换Survivor 区域。
如果一个对象足够 Old(在多次GC后还存活) 或Survivor2已满，则将其移动到老年代中。
最后，当老年代接近full时，将调用full GC。
在Spark中进行GC调优的目标是确保在老年代中只存储长期存在的 RDD，而年轻代的大小足以存储短期存在的对象。这将有助于避免在任务执行期间发生 full gc 来收集创建的临时对象。一些可能有用的步骤是:

通过收集GC统计信息来检查是否有太多的 GC 发生。如果在任务完成之前多次调用 full GC，这意味着没有足够的可用内存来执行任务。

如果 Minor GC 太多而 Major GC 太少，那么为Eden分配更多的内存会有所帮助。你可以将Eden的大小设置为高估 (over-estimate) 每个 task 所需的内存大小。如果 Eden 的大小被确定为 E，那么你可以使用选项 -Xmn=4/3*E 来设置年轻代的大小。（4/3的比例也考虑到了 Survivor 区域使用的空间, 一个 Survivor 占年轻代 1/8 空间）
在打印的GC统计中，如果老年代空间接近满，通过降低 spark.memory.fraction 来减少用于缓存的内存数量; 缓存更少的对象比降低任务执行速度要好。或者，考虑减少年轻代的规模。这意味着降低 -Xmn，如果你已经设置为上述。如果没有，请尝试更改JVM的 NewRatio 参数的值。许多 JVM 将其默认为2，这意味着老一代占用了2/3的堆。它应该足够大，使这个 fraction 超过 spark.memory.fraction。

尝试使用JVM参数 -XX:+UseG1GC 使用的 G1GC 垃圾收集器。在垃圾收集成为瓶颈的某些情况下，它可以提高性能。注意， 对于 Executor 堆大小容量大的集群, 用 -XX:G1HeapRegionSize 参数增加 G1 区域尺寸 会很重要。

例如，如果你的任务正在从HDFS读取数据，则可以使用从HDFS读取的数据块的大小来估计任务所使用的内存量。 注意，解压缩块的大小通常是块大小的2到3倍。因此，如果我们希望有3或4个任务的工作空间，并且 HDFS 块大小为128MB，我们可以估计Eden的大小为 4*3*128MB。
监视随新设置的变化, GC 所花费的频率和时间。

经验表明，GC调优的效果取决于你的应用程序和可用的内存量。官网描述了更多的调优选项，但是在高层次上，管理 full GC 发生的频率有助于减少开销。

可以通过在 Job 配置中，设置 spark.executor.extraJavaOptions 来指定 Executor 的GC调优等级。

## 3.数据倾斜

### 3.1.数据倾斜产生的原因及现象

对于大数据来说,数据量大不可怕,可怕的是数据倾斜

数据分布(key)不均匀 ==> 造成数据大量的集中在某个点上,造成数据热点问题

#### 3.1.1数据倾斜

数据倾斜是我们在处理大数据量问题时绕不过去的问题，也是在面试中几乎必问的考点。正常的数据分布理论上都是倾斜的，就是我们所说的'二八原理'：80%的财富集中在20%的人手中, 80%的用户只使用20%的功能 , 20%的用户贡献了80%的访问量。简单来说数据倾斜就是数据的key 的分化严重不均，造成一部分数据很多，一部分数据很少的局面。

**表现:**

数据倾斜会发生在数据开发的各个环节中，比如：用Hive算数据的时候reduce阶段卡在99.99%用SparkStreaming做实时算法时候，一直会有executor出现OOM的错误，但是其余的executor内存使用率却很低。

![表现](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210927145530.png)

**Hadoop**
当我们看任务进度长时间维持在99%，这里如果详细的看日志或者和监控界面的话会发现：

- 有一个多几个reduce卡住
- 各种container报错OOM
- 读写的数据量极大，至少远远超过其它正常的reduce
- 伴随着数据倾斜，会出现任务被kill等各种诡异的表现

**Spark**
Spark中的数据倾斜也很常见，Spark中一个 stage 的执行时间受限于最后那个执行完的 task，因此运行缓慢的任务会拖累整个程序的运行速度。过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。

**Flink**
使用Window、GroupBy、Distinct等聚合函数时，频繁出现反压，消费速度很慢，个别的task会出现OOM，调大资源也无济于事。

#### 3.1.2.数据倾斜原理和解决方案

在做数据运算的时候会设计到，countdistinct、group by、join等操作，都会触发Shuffle动作。
一旦触发，所有相同 key 的值就会拉到一个或几个节点上，发生单点问题。

一个简单的场景:

在订单表中，北京和上海两个地区的订单数量比其他地区高几个数量级。那么进行聚合的时候就会出现数据热点。

解决数据倾斜的几个思路：

1. 业务上：避免热点key的设计或者打散热点key，例如可以把北京和上海分成地区，然后单独汇总。
2. 技术上：在热点出现时，需要调整方案避免直接进行聚合，可以借助框架本身的能力，例如进行mapside-join。
3. 参数上：无论是Hadoop、Spark还是Flink都提供了大量的参数可以调整。

Hadoop/Hive参数

- mapside-join
- 对于group by或distinct，设定 hive.groupby.skewindata=true
- 合并小文件
- 压缩文件

Spark 参数

- 使用map join代替reduce join
- 提高shuffle并行度

Flink 参数

- MiniBatch设置
- 并行度设置

其他更多的是在业务上的key设计来避免。

### 3.2.MapReduce的shuffle

**Shuffle:**
Reducer的输入就是Mapper已经排好序的输出。在这个阶段，框架通过HTTP为每个Reducer获得所有Mapper输出中与之相关的分块。

**Sort:**
这个阶段，框架将按照key的值对Reducer的输入进行分组 （因为不同mapper的输出中可能会有相同的key）。

Shuffle和Sort两个阶段是同时进行的；map的输出也是一边被取回一边被合并的。

**Secondary Sort:**
如果需要中间过程对key的分组规则和reduce前对key的分组规则不同，那么可以通过 JobConf.setOutputValueGroupingComparator(Class)来指定一个Comparator。再加上 JobConf.setOutputKeyComparatorClass(Class)可用于控制中间过程的key如何被分组，所以结合两者可以实现按值的二次排序。

#### 3.2.1.概述

1. MapReduce 中，mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle
2. Shuffle: 数据混洗 ——（核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并 排序）
3. 具体来说：就是将 MapTask 输出的处理结果数据，按照 Partitioner 组件制定的规则分发 给 ReduceTask，并在分发的过程中，对数据按 key 进行了分区和排序

#### 3.2.2.MapReduce的Shuffle过程介绍

Shuffle的本义是洗牌、混洗，把一组有一定规则的数据尽量转换成一组无规则的数据，越随机越好。MapReduce中的Shuffle更像是洗牌的逆过程，把一组无规则的数据尽量转换成一组具有一定规则的数据。

为什么MapReduce计算模型需要Shuffle过程？我们都知道MapReduce计算模型一般包括两个重要的阶段：Map是映射，负责数据的过滤分发；Reduce是规约，负责数据的计算归并。Reduce的数据来源于Map，Map的输出即是Reduce的输入，Reduce需要通过Shuffle来获取数据。

从Map输出到Reduce输入的整个过程可以广义地称为Shuffle。Shuffle横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和sort过程，如图所示：

![mr过程](https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083504916-1942630366.png)

#### 3.2.3.Spill过程

Spill过程包括输出、排序、溢写、合并等步骤，如图所示：

![spill](https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083605120-770489646.png)

**Collect:**

每个Map任务不断地以对的形式把数据输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。

这个数据结构其实就是个字节数组，叫Kvbuffer，名如其义，但是这里面不光放置了数据，还放置了一些索引数据，给放置索引数据的区域起了一个Kvmeta的别名，在Kvbuffer的一块区域上穿了一个IntBuffer（字节序采用的是平台自身的字节序）的马甲。数据区域和索引数据区域在Kvbuffer中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次Spill之后都会更新一次。初始的分界点是0，数据的存储方向是向上增长，索引数据的存储方向是向下增长，如图所示：

![环形缓冲区](https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083814970-49385953.png)

Kvbuffer的存放指针bufindex是一直闷着头地向上增长，比如bufindex初始值为0，一个Int型的key写完之后，bufindex增长为4，一个Int型的value写完之后，bufindex增长为8。

索引是对在kvbuffer中的索引，是个四元组，包括：value的起始位置、key的起始位置、partition值、value的长度，占用四个Int长度，Kvmeta的存放指针Kvindex每次都是向下跳四个“格子”，然后再向上一个格子一个格子地填充四元组的数据。比如Kvindex初始位置是-4，当第一个写完之后，(Kvindex+0)的位置存放value的起始位置、(Kvindex+1)的位置存放key的起始位置、(Kvindex+2)的位置存放partition的值、(Kvindex+3)的位置存放value的长度，然后Kvindex跳到-8位置，等第二个和索引写完之后，Kvindex跳到-32位置。

Kvbuffer的大小虽然可以通过参数设置，但是总共就那么大，和索引不断地增加，加着加着，Kvbuffer总有不够用的那天，那怎么办？把数据从内存刷到磁盘上再接着往内存写数据，把Kvbuffer中的数据刷到磁盘上的过程就叫Spill，多么明了的叫法，内存中的数据满了就自动地spill到具有更大空间的磁盘。

关于Spill触发的条件，也就是Kvbuffer用到什么程度开始Spill，还是要讲究一下的。如果把Kvbuffer用得死死得，一点缝都不剩的时候再开始Spill，那Map任务就需要等Spill完成腾出空间之后才能继续写数据；如果Kvbuffer只是满到一定程度，比如80%的时候就开始Spill，那在Spill的同时，Map任务还能继续写数据，如果Spill够快，Map可能都不需要为空闲空间而发愁。两利相衡取其大，一般选择后者。

Spill这个重要的过程是由Spill线程承担，Spill线程从Map任务接到“命令”之后就开始正式干活，干的活叫SortAndSpill，原来不仅仅是Spill，在Spill之前还有个颇具争议性的Sort。

#### 3.2.4.Sort & Spill

先把Kvbuffer中的数据按照partition值和key两个关键字升序排序，移动的只是索引数据，排序结果是Kvmeta中数据按照partition为单位聚集在一起，同一partition内的按照key有序。

Spill线程为这次Spill过程创建一个磁盘文件：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out”的文件。Spill线程根据排过序的Kvmeta挨个partition的把数据吐到这个文件中，一个partition对应的数据吐完之后顺序地吐下个partition，直到把所有的partition遍历完。一个partition在文件中对应的数据也叫段(segment)。

所有的partition对应的数据都放在这个文件里，虽然是顺序存放的，但是怎么直接知道某个partition在这个文件中存放的起始位置呢？强大的索引又出场了。有一个三元组记录某个partition对应的数据在这个文件中的索引：起始位置、原始数据长度、压缩之后的数据长度，一个partition对应一个三元组。然后把这些索引信息存放在内存中，如果内存中放不下了，后续的索引信息就需要写到磁盘文件中了：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out.index”的文件，文件中不光存储了索引数据，还存储了crc32的校验数据。(spill12.out.index不一定在磁盘上创建，如果内存（默认1M空间）中能放得下就放在内存中，即使在磁盘上创建了，和spill12.out文件也不一定在同一个目录下。)

每一次Spill过程就会最少生成一个out文件，有时还会生成index文件，Spill的次数也烙印在文件名中。索引文件和数据文件的对应关系如下图所示：

![spill](https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083927742-1030906351.png)

在Spill线程如火如荼的进行SortAndSpill工作的同时，Map任务不会因此而停歇，而是一无既往地进行着数据输出。Map还是把数据写到kvbuffer中，那问题就来了：只顾着闷头按照bufindex指针向上增长，kvmeta只顾着按照Kvindex向下增长，是保持指针起始位置不变继续跑呢，还是另谋它路？如果保持指针起始位置不变，很快bufindex和Kvindex就碰头了，碰头之后再重新开始或者移动内存都比较麻烦，不可取。Map取kvbuffer中剩余空间的中间位置，用这个位置设置为新的分界点，bufindex指针移动到这个分界点，Kvindex移动到这个分界点的-16位置，然后两者就可以和谐地按照自己既定的轨迹放置数据了，当Spill完成，空间腾出之后，不需要做任何改动继续前进。分界点的转换如下图所示：

![分界点](https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083949570-945173243.png)

Map任务总要把输出的数据写到磁盘上，即使输出数据量很小在内存中全部能装得下，在最后也会把数据刷到磁盘上。

#### 3.2.5.Merge

Map任务如果输出数据量很大，可能会进行好几次Spill，out文件和Index文件会产生很多，分布在不同的磁盘上。最后把这些文件进行合并的merge过程闪亮登场。

Merge过程怎么知道产生的Spill文件都在哪了呢？从所有的本地目录上扫描得到产生的Spill文件，然后把路径存储在一个数组里。Merge过程又怎么知道Spill的索引信息呢？没错，也是从所有的本地目录上扫描得到Index文件，然后把索引信息存储在一个列表里。到这里，又遇到了一个值得纳闷的地方。在之前Spill过程中的时候为什么不直接把这些信息存储在内存中呢，何必又多了这步扫描的操作？特别是Spill的索引数据，之前当内存超限之后就把数据写到磁盘，现在又要从磁盘把这些数据读出来，还是需要装到更多的内存中。之所以多此一举，是因为这时kvbuffer这个内存大户已经不再使用可以回收，有内存空间来装这些数据了。（对于内存空间较大的土豪来说，用内存来省却这两个io步骤还是值得考虑的。）

然后为merge过程创建一个叫file.out的文件和一个叫file.out.Index的文件用来存储最终的输出和索引。

一个partition一个partition的进行合并输出。对于某个partition来说，从索引列表中查询这个partition对应的所有索引信息，每个对应一个段插入到段列表中。也就是这个partition对应一个段列表，记录所有的Spill文件中对应的这个partition那段数据的文件名、起始位置、长度等等。

然后对这个partition对应的所有的segment进行合并，目标是合并成一个segment。当这个partition对应很多个segment时，会分批地进行合并：先从segment列表中把第一批取出来，以key为关键字放置成最小堆，然后从最小堆中每次取出最小的输出到一个临时文件中，这样就把这一批段合并成一个临时的段，把它加回到segment列表中；再从segment列表中把第二批取出来合并输出到一个临时segment，把其加入到列表中；这样往复执行，直到剩下的段是一批，输出到最终的文件中。

最终的索引数据仍然输出到Index文件中。

![mergfe](https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321084015633-2083307488.png)

Map端的Shuffle过程到此结束。

#### 3.2.6.Copy & Merge Sort

**Copy:**

Reduce任务通过HTTP向各个Map任务拖取它所需要的数据。每个节点都会启动一个常驻的HTTP server，其中一项服务就是响应Reduce拖取Map数据。当有MapOutput的HTTP请求过来的时候，HTTP server就读取相应的Map输出文件中对应这个Reduce部分的数据通过网络流输出给Reduce。

Reduce任务拖取某个Map对应的数据，如果在内存中能放得下这次数据的话就直接把数据写到内存中。Reduce要向每个Map去拖取数据，在内存中每个Map对应一块数据，当内存中存储的Map数据占用空间达到一定程度的时候，开始启动内存中merge，把内存中的数据merge输出到磁盘上一个文件中。

如果在内存中不能放得下这个Map的数据的话，直接把Map数据写到磁盘上，在本地目录创建一个文件，从HTTP流中读取数据然后写到磁盘，使用的缓存区大小是64K。拖一个Map数据过来就会创建一个文件，当文件数量达到一定阈值时，开始启动磁盘文件merge，把这些文件合并输出到一个文件。

有些Map的数据较小是可以放在内存中的，有些Map的数据较大需要放在磁盘上，这样最后Reduce任务拖过来的数据有些放在内存中了有些放在磁盘上，最后会对这些来一个全局合并。

**Merge Sort:**

这里使用的Merge和Map端使用的Merge过程一样。Map的输出数据已经是有序的，Merge进行一次合并排序，所谓Reduce端的sort过程就是这个合并的过程。一般Reduce是一边copy一边sort，即copy和sort两个阶段是重叠而不是完全分开的。

Reduce端的Shuffle过程至此结束。

### 3.3.Spark的shffle

#### 3.3.1.SparkShuffle概念

reduceByKey会将上一个RDD中的每一个key对应的所有value聚合成一个value，然后生成一个新的RDD，元素类型是<key,value>对的形式，这样每一个key对应一个聚合起来的value。

![zhanshipng](https://github.com/me281026/Img-Repo/blob/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210927154710.png?raw=true)

**问题：** 聚合之前，每一个key对应的value不一定都是在一个partition中，也不太可能在同一个节点上，因为RDD是分布式的弹性的数据集，RDD的partition极有可能分布在各个节点上。

**如何聚合？**

- Shuffle Write：上一个stage的每个map task就必须保证将自己处理的当前分区的数据相同的key写入一个分区文件中，可能会写入多个不同的分区文件中。
- Shuffle Read：reduce task就会从上一个stage的所有task所在的机器上寻找属于己的那些分区文件，这样就可以保证每一个key所对应的value都会汇聚到同一个节点上去处理和聚合。

Spark中有两种Shuffle类型，HashShuffle和SortShuffle，Spark1.2之前是HashShuffle默认的分区器是HashPartitioner，Spark1.2引入SortShuffle默认的分区器是RangePartitioner。

#### 3.3.2.HashShuffle

**普通机制:**

普通机制示意图

![普通机制示意图](https://pic4.zhimg.com/v2-01553e6f5ddc122f4b4bea1526a0eb73_b.jpg)

**执行流程:**

1. 每一个map task将不同结果写到不同的buffer中，每个buffer的大小为32K。buffer起到数据缓存的作用。
2. 每个buffer文件最后对应一个磁盘小文件。
3. reduce task来拉取对应的磁盘小文件。

**总结:**

1. map task的计算结果会根据分区器（默认是hashPartitioner）来决定写入到哪一个磁盘小文件中去。ReduceTask会去Map端拉取相应的磁盘小文件。
2. 产生的磁盘小文件的个数：M（map task的个数）*R（reduce task的个数）

**存在的问题**
产生的磁盘小文件过多，会导致以下问题：

1. 在Shuffle Write过程中会产生很多写磁盘小文件的对象。
2. 在Shuffle Read过程中会产生很多读取磁盘小文件的对象。
3. 在JVM堆内存中对象过多会造成频繁的gc,gc还无法解决运行所需要的内存 的话，就会OOM。
4. 在数据传输过程中会有频繁的网络通信，频繁的网络通信出现通信故障的可能性大大增加，一旦网络通信出现了故障会导致shuffle file cannot find 由于这个错误导致的task失败，TaskScheduler不负责重试，由DAGScheduler负责重试Stage。

**合并机制:**

合并机制示意图

![合并机制示意图](https://pic2.zhimg.com/v2-57544c643c2d4ca1b972cecf06c38095_b.jpg)

**总结:**
产生磁盘小文件的个数：C(core的个数)*R（reduce的个数）

#### 3.3.3.SortShuffle

**普通机制:**

普通机制示意图

![普通机制示意图](https://pic2.zhimg.com/v2-c90c0720ba43e4900b271186408de785_b.jpg)

**执行流程:**

1. map task 的计算结果会写入到一个内存数据结构里面，内存数据结构默认是5M
2. 在shuffle的时候会有一个定时器，不定期的去估算这个内存结构的大小，当内存结构中的数据超过5M时，比如现在内存结构中的数据为5.01M，那么他会申请5.01*2-5=5.02M内存给内存数据结构。
3. 如果申请成功不会进行溢写，如果申请不成功，这时候会发生溢写磁盘。
4. 在溢写之前内存结构中的数据会进行排序分区
5. 然后开始溢写磁盘，写磁盘是以batch的形式去写，一个batch是1万条数据
6. map task执行完成后，会将这些磁盘小文件合并成一个大的磁盘文件，同时生成一个索引文件。
7. reduce task去map端拉取数据的时候，首先解析索引文件，根据索引文件再去拉取对应的数据。

**总结:**

产生磁盘小文件的个数： 2*M（map task的个数）

**bypass机制:**

bypass机制示意图

![bypass机制示意图](https://pic2.zhimg.com/v2-97800e7fdae42a4e2bce714910f71de1_b.jpg)

**总结:**

1. bypass运行机制的触发条件如下：shuffle reduce task的数量小于spark.shuffle.sort.bypassMergeThreshold的参数值。这个值默认是200
2. 产生的磁盘小文件为：2*M（map task的个数）

**Shuffle文件寻址:**

MapOutputTrackerMapOutputTracker是Spark架构中的一个模块，是一个主从架构。管理磁盘小文件的地址。
MapOutputTrackerMaster是主对象，存在于Driver中。
MapOutputTrackerWorker是从对象，存在于Excutor中。

![Shuffle文件寻址图](https://pic2.zhimg.com/v2-b95d14b92c37c2a936369962206f53e9_b.jpg)

**BlockManager:**

BlockManager块管理者，是Spark架构中的一个模块，也是一个主从架构。
BlockManagerMaster,主对象，存在于Driver中。
BlockManagerMaster会在集群中有用到广播变量和缓存数据或者删除缓存数据的时候，通知BlockManagerSlave传输或者删除数据。
BlockManagerWorker，从对象，存在于Excutor中。BlockManagerWorker会与BlockManagerWorker之间通信。

无论在Driver端的BlockManager还是在Excutor端的BlockManager都含有四个对象：

1. DiskStore:负责磁盘的管理。
2. MemoryStore：负责内存的管理。
3. ConnectionManager：负责连接其他的BlockManagerWorker。
4. BlockTransferService:负责数据的传输。

#### 3.3.4.宽窄依赖

RDD之间有一系列的依赖关系，依赖关系又分为窄依赖和宽依赖。

Spark中的Stage其实是一组并行的任务，任务是一个个的Task

**窄依赖：**

父RDD和子RDDpartition之间的关系是一对一的，或者父RDD一个partition只对应一个子RDD的partition情况下的父RDD和子RDD partition关系是多对一的，不会有shuffle产生。父RDD的一个分区去到了子RDD的一个分区

![窄依赖](https://pic2.zhimg.com/v2-92a6e538e69e3ef92aca7278288ff541_b.jpg)

**宽依赖：**

父RDD与子RDD partition之间的关系是一对多，会有shuffle的产生。父RDD的一个分区的数据去到了子RDD的不同分区里面。

![宽依赖](https://pic2.zhimg.com/v2-92a6e538e69e3ef92aca7278288ff541_b.jpg)

区分宽窄依赖主要就是看父RDD的一个partition的流向，要是流向一个的话就是窄依赖，流向多个的话就是宽依赖。

相比于宽依赖，窄依赖对优化很有利，主要基于一下两点：

1. 宽依赖往往对应着shuffle操作，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点间的数据传输，而窄依赖的每个父RDD分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换。
2. 当RDD分区丢失时（某个节点故障），spark会对数据进行重算
   1. 对于窄依赖，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重算和子RDD分区对应的父RDD分区即可，所以这个重算对数据的利用率是100%的。
   2. 对于宽依赖，重算的父RDD分区对应多个字RDD分区，这样实际上父RDD中只有一部分的数据是被用于恢复这个丢失的子RDD分区的，另一部分对应子RDD的其他未丢失分区，这就造成了多余的计算，宽依赖中子RDD分区通常来自于多个父RDD分区，极端情况下，所有的父RDD分区都要重新计算
   3. 如下图所示，b1分区丢失，则需要重新计算a1，a2和a3，这样就产生了冗余计算（a1,a2,a3中对应着b2的数据）

![区分依赖](https://pic2.zhimg.com/v2-4a892021e1c32efeeaa2fbe76d19d455_b.jpg)

区分这两种依赖很有用，首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。

例如，逐个元素地执行map，然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点间进行shuffle，这和MapReduce类似。

第二，窄依赖能够更有效地进行失效节点的恢复，即只需要重新计算丢失RDD分区的父分区，而且不同节点间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。

在深入分区级别来看待这个问题，重算的效用并不在于算了多少，而是在于有多少是冗余的计算。窄依赖中需要重算的都是必须的，所以重算并不会产生冗余计算。

### 3.4.数据倾斜的场景

#### 3.4.1.造成数据倾斜的原因

基本业务逻辑是造成数据倾斜的主要原因：

1. group by逻辑造成
2. distinct count(distinct xx)
3. 小表join大表
4. 大表join大表

#### 3.4.2.解决方案

**group by解决方案:**

Hive做group by查询，当遇到group by字段的某些值特别多的时候，会将相同值拉到同一个reduce任务进行聚合，也容易发生数据倾斜。

优化方法：

**1)开启Map端聚合:**

set hive.map.aggr=true;
开启map端聚合，效率更高但需要更多的内存

set hive.groupby.skewindata=true;
开启group by数据倾斜时负载均衡，生成的查询计划会有两个MRJob:

1. 第一个MRJob 中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；
2. 第二个MRJob再根据预处理的数据结果按照GroupBy Key分布到Reduce中（这个过程可以保证相同的GroupBy Key被分布到同一个Reduce中），最后完成最终的聚合操作。

**2)有数据倾斜时进行负载均衡:**

当设定hive.groupby.skewindata为true时，生成的查询计划会有两个MapReduce任务:

1. 在第一个MapReduce 中，map的输出结果集合会随机分布到 reduce 中， 每个 reduce 做部分聚合操作，这样处理之后，相同的 Group By Key 有可能分发到不同的 reduce 中，从而达到负载均衡的目的。
2. 在第二个 MapReduce 任务再根据第一步中处理的数据按照Group By Key分布到reduce中，（这一步中相同的key在同一个reduce中），最终生成聚合操作结果。

**COUNT DISTINCT:**

当在数据量比较大的情况下，由于COUNT DISTINCT操作是用一个reduce任务来完成，
这一个reduce需要处理的数据量太大，就会导致整个job很难完成，这也可以归纳为一种数据倾斜。

优化方法：将COUNT DISTINCT使用先GROUP BY再COUNT的方式替换。

例如：

```sql
select count(id) from (select id from bigtable group by id) a
```

因此，count distinct的优化本质上也是转成group by操作。

**小表与大表JOIN:**

小表与大表Join时容易发生数据倾斜，表现为小表的数据量比较少但key却比较集中

导致分发到某一个或几个reduce上的数据比其他reduce多很多，造成数据倾斜。

优化方法：

使用Map Join将小表装入内存，在map端完成join操作，这样就避免了reduce操作。有两种方法可以执行Map Join：

**(1) 通过hint指定小表做MapJoin:**

```sql
 select /*+ MAPJOIN(time_dim) */ count(*) from store_sales join time_dim on ss_sold_time_sk = t_time_sk;
```

**(2) 通过配置参数自动做MapJoin:**

核心参数：

因此，巧用MapJoin可以有效解决小表关联大表场景下的数据倾斜。

set hive.auto.convert.join = true
如果是小表，自动选择map join:

hive.mapjoin.smalltable.filesize=25000000
调大小表的阈值，默认值是25mb

**大表与大表JOIN:**

大表与大表Join时，当其中一张表的NULL值（或其他值）比较多时，容易导致这些相同值在reduce阶段集中在某一个或几个reduce上，发生数据倾斜问题。

优化方法：

(1) 将NULL值提取出来最后合并，这一部分只有map操作；非NULL值的数据分散到不同reduce上，不会出现某个reduce任务数据加工时间过长的情况，整体效率提升明显。这种方法由于有两次Table Scan会导致map增多。

```sql

SELECT a.user_Id,a.username,b.customer_id
  FROM user_info a
  LEFT JOIN customer_info b
  ON a.user_id = b.user_id
  where a.user_id IS NOT NULL
  UNION ALL
  SELECT a.user_Id,a.username,NULL
  FROM user_info a
  WHERE a.user_id IS NULL

```

(2) 在Join时直接把NULL值打散成随机值来作为reduce的key值，不会出现某个reduce任务数据加工时间过长的情况，整体效率提升明显。这种方法解释计划只有一次map，效率一般优于第一种方法。

```sql

SELECT a.user_id,a.username,b.customer_id
  FROM user_info a
  LEFT JOIN customer_info b
  ON
  CASE WHEN
   a.user_id IS NULL
  THEN
   CONCAT ('dp_hive', RAND())
  ELSE
   a.user_id
  END = b.user_id;

```

Map输出key数量极少，导致reduce端退化为单机作业
考虑先对Join中的一个表去重，以此结果过滤无用信息，考虑先对Join中的一个表去重，以此结果过滤无用信息

Map输出key分布不均，少量key对应大量value，导致reduce端单机瓶颈
