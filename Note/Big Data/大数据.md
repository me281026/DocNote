# 大数据

## 1.Hadoop

### 1.1.HDFS

#### 1.1.1.Namenode 和 Datanode

HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。

#### 1.1.2.HDFS架构

![hdfs](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)

#### 1.1.3.Namespace

HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。

Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由Namenode保存的。

#### 1.1.4.Data Replication

HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。

Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。

![数据复制](https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsdatanodes.gif)

**存放机制:** 副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。

在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。

#### 1.1.5.组件功能

![组件](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632294496.jpg)

#### 1.1.6.HDFS写流程

![写](https://pic4.zhimg.com/v2-6de9f191ab1dce72dcda3f5aa2d3b33f_b.jpg)

1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。
2. NameNode返回是否可以上传。
3. 客户端请求第一个 Block上传到哪几个DataNode服务器上。
4. NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。
5. 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。
6. dn1、dn2、dn3逐级应答客户端。
7. 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。
8. 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。

---
NameNode：可以理解为DataNode管理器
DataNode：存储块数据，默认128M为一块

**网络拓扑-节点距离计算** :在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？

节点距离：两个节点到达最近的共同祖先的距离总和。

不好理解的就是共同祖先啥意思，可以理解为上级节点，节点等级如下
【数据中心（集群d）--->  机架r ---> 具体节点n】
举个例子：计算节点d1/r1/n1到节点d1/r1/n2的节点距离
确认共同祖先为r1，节点n1到它的距离为1，节点n2到它的距离也为1，两者和为2
所以它们之间的节点距离就是2

![距离计算](https://pic4.zhimg.com/v2-7fdaf587e5b47b8deede56d235c229f3_b.jpg)

**机架感知** :副本节点的位置的选择就行

![位置选择](https://pic2.zhimg.com/v2-44f72ca2fc3214c48d02958241177d89_b.jpg)

---

#### 1.1.7.HDFS读流程

![读](https://pic3.zhimg.com/v2-811b2a0e2bc91542e195da86d299a9a2_b.jpg)

1. 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

#### 1.1.8.HDFS的HA

![HA](https://img-blog.csdn.net/20180821142402638?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzI4OTk3NjU1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Namenode在HDFS中是一个非常重要的组件，相当于HDFS文件系统的心脏，在显示分布式集群环境中，还是会有可能出现Namenode的崩溃或各种意外。所以，高可用模式就体现出作用了。
namenode HA配置大概流程：

- 在启动namenode之前，需要启动hadoop2.x中新引入的（QJM）Quorum Journal Manager，QJM主要用来管理namenode之间的数据同步，当active namenode数据更新时会传递给QJM，QJM在所有的namenode之间同步，最后QJM将active namenode 更新的数据同步到了standby namenode中。
- 启动多个namenode时，并配置namenode的主机地址，还要配置隔离机制，因为容易出现SB（split-brain）状况，所谓的sb状况意思就是当多个namenode正常状态时，一台active，多台standby。如果某段时间因为网络等非namenode自身关系导致namenode间交流阻断了，这样容易出现多台active的设备，容易抢占资源等。
- 引入zookeeper来对namenode进行监听，因为在一般情况下，active 的namenode崩溃了的话，需要人工切换standby Namenode为active。非常不人性化。通过zookeeper可以监听多个namenode，当active namenode崩溃的话，zookeeper监听到后马上通知zookeeper的leader进行主备选举，在standby namenode中选举出一台，并将它置为active模式替换崩溃的namenode。

#### 1.1.9.小文件

Hadoop中的小文件一般是指明显小于HDFS的block size（默认128M，一般整数倍配置如256M）的文件。但需要注意，HDFS上的有些小文件是不可避免的，比如jar、临时缓存文件等。但当小文件数量变的"海量"，以至于Hadoop集群中存储了大量的小文件，就需要对小文件进行处理，而处理的目标是让文件大小尽可能接近HDFS的block size大小或者整数倍。

- 明显小于block size文件的80%
- 129M:128M+1M

##### Hadoop小文件带来的问题

1. 众所周知，在HDFS中数据和元数据分别由DataNode和NameNode负责，这些元数据每个对象一般占用大约150个字节。大量的小文件相对于大文件会占用大量的NameNode内存。对NameNode内存管理产生巨大挑战，此外对JVM稳定性也有影响如GC。
2. 当NameNode重启时，它需要将文件系统元数据从本地磁盘加载到内存中。如果NameNode的元数据很大，重启速度会非常慢。
3. 一般来说，NameNode会不断跟踪并检查集群中每个block块的存储位置。这是通过DataNode的定时心跳上报其数据块来实现的。数据节点需要上报的block越多，则也会消耗越多的网络带宽/时延。
4. 更多的文件意味着更多的读取请求需要请求NameNode，这可能最终会堵塞NameNode的容量，增加RPC队列和处理延迟，进而导致性能和响应能力下降。
5. 对计算引擎如Spark、MapReduce性能造成负面影响。以MapReduce（以下简称MR）为例，大量小文件意味着大量的磁盘IO，磁盘IO通常是MR性能的最大瓶颈之一，在HDFS中对于相同数量的数据，一次大的顺序读取往往优于几次随机读取的性能。如果可以将数据存储在较少，而更大的一些block中，可以降低磁盘IO的性能影响。除了磁盘IO，还有内部任务的划分、资源分配等

##### Hadoop小文件是怎么来的

一个Hadoop集群中存在小文件的可能原因如下：

1. 流式任务（如spark streaming/flink等实时计算框架）在做数据处理时，无论是纯实时还是基于batch的准实时，在小的时间窗口内都可能产生大量的小文件。此外对于Spark任务如果过度并行化，每个分区一个文件，产生的文件也可能会增多
2. Hive分区表的过度分区这里的过度分区是指Hive分区表的每个分区数据量很小（比如小于HDFS block size）的Hive表。那么Hive Metastore Server调用开销会随着表拥有的分区数量而增加，影响性能。此时，要衡量数据量重新进行表结构设计（如减少分区粒度）。
3. 数据源有大量小文件，未做处理直接迁移到Hadoop集群。
4. 对于计算引擎处理任务，以MR为例。大量的map和reduce task存在。在HDFS上生成的文件基本上与map数量（对于Map-Only作业）或reduce数量（对于MR作业）成正比。此外，MR任务如果未设置合理的reduce数或者未做限制，每个reduce都会生成一个独立的文件。对于数据倾斜，导致大部分的数据都shuffle到一个或几个reduce，然后其他的reduce都会处理较小的数据量并输出小文件。对于Spark任务，过度并行化也是导致小文件过多的原因之一。在Spark作业中，根据写任务中提到的分区数量，每个分区会写一个新文件。这类似于MapReduce框架中的每个reduce任务都会创建一个新文件。Spark分区越多，写入的文件就越多。控制分区的数量来减少小文件的生成。

### 1.2.SQL On Hadoop

#### 1.2.1.SQL和存储

sql ==> 对应执行引擎的作业:MapReduce/Spark/Tez

Impala:内存
Presto:
Drill:
Phoenix:HBase(rowkey)
Spark SQL:

MetaStore:存储元数据,框架之间是共享元数据信息的

行式存储/列式存储

![C/R](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632376364(1).jpg)

#### 1.2.2.调优策略

1. 架构
   - 分表
   - 分区表(partition)
   - 中间结果集
   - 压缩
2. 语法
   - 排序:order by/partition by/distribute by/cluster by
   - 控制输出(reduce/partition/task)数量
   - join(普通join/map join)
   - 执行计划
3. 执行
   - 推测执行
   - 并行执行
   - JVM重用

**分表:**

![分表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377124(1).jpg)

**分区表:**

![分区表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377381(1).jpg)

**中间结果集:**

![中间结果](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377645(1).jpg)

**压缩:**

![压缩](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377929(1).jpg)

![压缩详解](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632378025(1).jpg)
