# 大数据

## 1.Hadoop

### 1.1.HDFS

#### 1.1.1.Namenode 和 Datanode

HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。

#### 1.1.2.HDFS架构

![hdfs](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)

#### 1.1.3.Namespace

HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。

Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由Namenode保存的。

#### 1.1.4.Data Replication

HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。

Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。

![数据复制](https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsdatanodes.gif)

**存放机制:** 副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。

在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。

#### 1.1.5.组件功能

![组件](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632294496.jpg)

#### 1.1.6.HDFS写流程

![写](https://pic4.zhimg.com/v2-6de9f191ab1dce72dcda3f5aa2d3b33f_b.jpg)

1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。
2. NameNode返回是否可以上传。
3. 客户端请求第一个 Block上传到哪几个DataNode服务器上。
4. NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。
5. 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。
6. dn1、dn2、dn3逐级应答客户端。
7. 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。
8. 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。

---
NameNode：可以理解为DataNode管理器
DataNode：存储块数据，默认128M为一块

**网络拓扑-节点距离计算** :在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？

节点距离：两个节点到达最近的共同祖先的距离总和。

不好理解的就是共同祖先啥意思，可以理解为上级节点，节点等级如下
【数据中心（集群d）--->  机架r ---> 具体节点n】
举个例子：计算节点d1/r1/n1到节点d1/r1/n2的节点距离
确认共同祖先为r1，节点n1到它的距离为1，节点n2到它的距离也为1，两者和为2
所以它们之间的节点距离就是2

![距离计算](https://pic4.zhimg.com/v2-7fdaf587e5b47b8deede56d235c229f3_b.jpg)

**机架感知** :副本节点的位置的选择就行

![位置选择](https://pic2.zhimg.com/v2-44f72ca2fc3214c48d02958241177d89_b.jpg)

---

#### 1.1.7.HDFS读流程

![读](https://pic3.zhimg.com/v2-811b2a0e2bc91542e195da86d299a9a2_b.jpg)

1. 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

#### 1.1.8.HDFS的HA

![HA](https://img-blog.csdn.net/20180821142402638?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzI4OTk3NjU1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Namenode在HDFS中是一个非常重要的组件，相当于HDFS文件系统的心脏，在显示分布式集群环境中，还是会有可能出现Namenode的崩溃或各种意外。所以，高可用模式就体现出作用了。
namenode HA配置大概流程：

- 在启动namenode之前，需要启动hadoop2.x中新引入的（QJM）Quorum Journal Manager，QJM主要用来管理namenode之间的数据同步，当active namenode数据更新时会传递给QJM，QJM在所有的namenode之间同步，最后QJM将active namenode 更新的数据同步到了standby namenode中。
- 启动多个namenode时，并配置namenode的主机地址，还要配置隔离机制，因为容易出现SB（split-brain）状况，所谓的sb状况意思就是当多个namenode正常状态时，一台active，多台standby。如果某段时间因为网络等非namenode自身关系导致namenode间交流阻断了，这样容易出现多台active的设备，容易抢占资源等。
- 引入zookeeper来对namenode进行监听，因为在一般情况下，active 的namenode崩溃了的话，需要人工切换standby Namenode为active。非常不人性化。通过zookeeper可以监听多个namenode，当active namenode崩溃的话，zookeeper监听到后马上通知zookeeper的leader进行主备选举，在standby namenode中选举出一台，并将它置为active模式替换崩溃的namenode。

#### 1.1.9.小文件

Hadoop中的小文件一般是指明显小于HDFS的block size（默认128M，一般整数倍配置如256M）的文件。但需要注意，HDFS上的有些小文件是不可避免的，比如jar、临时缓存文件等。但当小文件数量变的"海量"，以至于Hadoop集群中存储了大量的小文件，就需要对小文件进行处理，而处理的目标是让文件大小尽可能接近HDFS的block size大小或者整数倍。

- 明显小于block size文件的80%
- 129M:128M+1M

##### Hadoop小文件带来的问题

1. 众所周知，在HDFS中数据和元数据分别由DataNode和NameNode负责，这些元数据每个对象一般占用大约150个字节。大量的小文件相对于大文件会占用大量的NameNode内存。对NameNode内存管理产生巨大挑战，此外对JVM稳定性也有影响如GC。
2. 当NameNode重启时，它需要将文件系统元数据从本地磁盘加载到内存中。如果NameNode的元数据很大，重启速度会非常慢。
3. 一般来说，NameNode会不断跟踪并检查集群中每个block块的存储位置。这是通过DataNode的定时心跳上报其数据块来实现的。数据节点需要上报的block越多，则也会消耗越多的网络带宽/时延。
4. 更多的文件意味着更多的读取请求需要请求NameNode，这可能最终会堵塞NameNode的容量，增加RPC队列和处理延迟，进而导致性能和响应能力下降。
5. 对计算引擎如Spark、MapReduce性能造成负面影响。以MapReduce（以下简称MR）为例，大量小文件意味着大量的磁盘IO，磁盘IO通常是MR性能的最大瓶颈之一，在HDFS中对于相同数量的数据，一次大的顺序读取往往优于几次随机读取的性能。如果可以将数据存储在较少，而更大的一些block中，可以降低磁盘IO的性能影响。除了磁盘IO，还有内部任务的划分、资源分配等

##### Hadoop小文件是怎么来的

一个Hadoop集群中存在小文件的可能原因如下：

1. 流式任务（如spark streaming/flink等实时计算框架）在做数据处理时，无论是纯实时还是基于batch的准实时，在小的时间窗口内都可能产生大量的小文件。此外对于Spark任务如果过度并行化，每个分区一个文件，产生的文件也可能会增多
2. Hive分区表的过度分区这里的过度分区是指Hive分区表的每个分区数据量很小（比如小于HDFS block size）的Hive表。那么Hive Metastore Server调用开销会随着表拥有的分区数量而增加，影响性能。此时，要衡量数据量重新进行表结构设计（如减少分区粒度）。
3. 数据源有大量小文件，未做处理直接迁移到Hadoop集群。
4. 对于计算引擎处理任务，以MR为例。大量的map和reduce task存在。在HDFS上生成的文件基本上与map数量（对于Map-Only作业）或reduce数量（对于MR作业）成正比。此外，MR任务如果未设置合理的reduce数或者未做限制，每个reduce都会生成一个独立的文件。对于数据倾斜，导致大部分的数据都shuffle到一个或几个reduce，然后其他的reduce都会处理较小的数据量并输出小文件。对于Spark任务，过度并行化也是导致小文件过多的原因之一。在Spark作业中，根据写任务中提到的分区数量，每个分区会写一个新文件。这类似于MapReduce框架中的每个reduce任务都会创建一个新文件。Spark分区越多，写入的文件就越多。控制分区的数量来减少小文件的生成。

### 1.2.SQL On Hadoop

#### 1.2.1.SQL和存储

sql ==> 对应执行引擎的作业:MapReduce/Spark/Tez

Impala:内存
Presto:
Drill:
Phoenix:HBase(rowkey)
Spark SQL:

MetaStore:存储元数据,框架之间是共享元数据信息的

行式存储/列式存储

![C/R](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632376364(1).jpg)

#### 1.2.2.调优策略

1. 架构
   - 分表
   - 分区表(partition)
   - 中间结果集
   - 压缩
2. 语法
   - 排序:order by/partition by/distribute by/cluster by
   - 控制输出(reduce/partition/task)数量
   - join(普通join/map join)
   - 执行计划
3. 执行
   - 推测执行
   - 并行执行
   - JVM重用

##### 1. 架构

**分表:**

![分表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377124(1).jpg)

**分区表:**

![分区表](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377381(1).jpg)

**中间结果集:**

![中间结果](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377645(1).jpg)

**压缩:**

![压缩](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632377929(1).jpg)

![压缩详解](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632378025(1).jpg)

##### 2. 语法

**Order by / Sort by:**

Hive QL 中的\* ORDER BY 语法类似于 SQL 语言中的 ORDER BY \*语法。

---

colOrder: ( ASC | DESC )
colNullOrder: (NULLS FIRST | NULLS LAST)           -- (Note: Available in Hive 2.1.0 and later)
orderBy: ORDER BY colName colOrder? colNullOrder? (',' colName colOrder? colNullOrder?)*
query: SELECT expression (',' expression)* FROM src orderBy

---

order by子句中有一些限制。在严格模式下(即hive.mapred.mode = strict)，必须在 order by 子句后跟一个limit子句。如果将 hive.mapred.mode 设置为 nonstrict，则没有 limit 子句。原因是为了强加所有结果的总 Sequences，必须有一个减速器才能对最终输出进行排序。如果输出中的行数太大，则单个减速器可能需要很长时间才能完成。

对于 Hive 0.11.0 到 2.1.x，将hive.groupby.orderby.position.alias设置为 true(默认值为 false)。

对于 Hive 2.2.0 和更高版本，默认情况下hive.orderby.position.alias为 true。

SORT BY 语法类似于 SQL 语言中的 ORDER BY *语法。

---

colOrder: ( ASC | DESC )
sortBy: SORT BY colName colOrder? (',' colName colOrder?)*
query: SELECT expression (',' expression)* FROM src sortBy

---

Hive 使用\* SORT BY *中的列对行进行排序，然后再将行 Importing 到 reducer。排序 Sequences 将取决于列类型。如果列是数字类型，则排序 Sequences 也是数字 Sequences。如果列是字符串类型，则排序 Sequences 将是字典 Sequences。

在Hive 3.0.0及更高版本中，优化器将删除subqueries和views中的无限制排序。要禁用它，请将hive.remove.orderby.in.subquery设置为 false。

Hive 支持\* SORT BY *，可对每个 reducer 的数据进行排序。 “ order by”和“ sort by”之间的区别在于，前者保证输出中的总 Sequences，而后者仅保证精简器中行的排序。如果存在多个减速器，则“排序依据”可能会给出部分排序的最终结果。

注意：关于单个列的单独 SORT BY 与 CLUSTER BY 之间的区别可能会造成混淆。不同之处在于，如果有多个 reducer 分区，则 CLUSTER BY 按字段划分，而 SORT BY 则是随机划分，以便在 reducer 上均匀地分布数据(和负载)。

基本上，每个 reducer 中的数据将根据用户指定的 Sequences 进行排序。

**Distribute by/Cluster by:**

Cluster By 和 Distribute By *主要与Transform/Map-Reduce Scripts一起使用。但是，如果需要对查询的输出进行分区和排序以用于后续查询，有时在 SELECT 语句中很有用。

Cluster By 是 Distribute By 和 Sort By *的快捷方式。

Hive 使用\* Distribute By 中的列在reducer之间分配行。具有相同 Distribute By 列的所有行将进入相同的 reducer。但是， Distribute By *不保证分布式键上的聚类或排序属性。

---

例如，我们将以下 5 行的* x 分配给 2 个 reducer：

x1
x2
x4
x3
x1

==reducer 1==

x1
x2
x1

==reducer 2==

x4
x3
请注意，具有相同键 x1 的所有行都可以保证分配给同一reducer(在这种情况下为reducer 1)，但不能保证它们聚集在相邻位置。

相反，如果我们使用\* Cluster By x *，则两个化简器将进一步对 x 上的行进行排序：

==reducer 1==

x1
x1
x2

==reducer 2==

x3
x4
用户可以指定\* Distribute By 和 Sort By 来代替 Cluster By *，因此分区列和排序列可以不同。通常的情况是分区列是排序列的前缀，但这不是必需的。

```SQL
SELECT col1, col2 FROM t1 CLUSTER BY col1
SELECT col1, col2 FROM t1 DISTRIBUTE BY col1

SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC
FROM (
    FROM pv_users
    MAP ( pv_users.userid, pv_users.date )
    USING 'map_script'
    AS c1, c2, c3
    DISTRIBUTE BY c2
    SORT BY c2, c1) map_output
  INSERT OVERWRITE TABLE pv_users_reduced
    REDUCE ( map_output.c1, map_output.c2, map_output.c3 )
    USING 'reduce_script'
    AS date, count;
```

---

**总结:**

![区别](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632463246(1).jpg)

**执行计划(EXPLAIN):**

Hive 提供了一个EXPLAIN命令，该命令显示了查询的执行计划。该语句的语法如下：

EXPLAIN [EXTENDED|CBO|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE] query

HIVE 0.14.0 通过HIVE-5961支持AUTHORIZATION。 Hive 2.3.0 通过HIVE-11394支持VECTORIZATION。 Hive 3.2.0 通过HIVE-17683支持LOCKS。

AST 已从HIVE-13533中的 EXPLAIN EXTENDED 中删除，并在HIVE-15932中作为单独的命令恢复。

在EXPLAIN语句中使用EXTENDED会产生有关计划中运算符的更多信息。这通常是物理信息，例如文件名。

Hive 查询将转换为阶段序列(它更是有向非循环图)。这些阶段可以是 map/reduce 阶段，或者甚至可以是执行元存储或文件系统操作(如移动和重命名)的阶段。说明输出分为三个部分：

查询的抽象语法树

计划不同阶段之间的依赖性

每个阶段的描述

这些阶段本身的描述显示了一系列操作符，以及与操作符相关联的元数据。元数据可能包含诸如 FilterOperator 的过滤器表达式或 SelectOperator 的选择表达式或 FileSinkOperator 的输出文件名之类的内容。

**JOIN:**

LEFT SEMI JOIN 以有效的方式实现了不相关的 IN/EXISTS 子查询语义。从 Hive 0.13 开始，使用subqueries支持 IN/NOT IN/EXISTS/NOT EXISTS 运算符，因此这些 JOIN 中的大多数不再需要手动执行。使用 LEFT SEMI JOIN 的限制是只能在连接条件(ON 子句)中引用右侧表，而不能在 WHERE 或 SELECT 子句等中引用。

如果除一个要连接的表之外的所有表都很小，则可以将其作为仅 Map 作业执行。查询

```SQL

SELECT /*+ MAPJOIN(b) */ a.key, a.value
  FROM a JOIN b ON a.key = b.key

不需要reduce。对于 A 的每个 Map 器，B 都会被完全读取。
```

不支持以下内容。

union 后跟一个 MapJoin

横向视图后跟一个 MapJoin

减少接收器(分组依据/加入/排序依据/集群依据/分发依据)，其次是 MapJoin

MapJoin 之后是 union

MapJoin，然后加入

MapJoin 其次是 MapJoin

配置变量 hive.auto.convert.join(如果设置为 true)会在运行时自动将联接转换为 mapjoins，应使用它代替 mapjoin 提示。 mapjoin 提示仅应用于以下查询。

如果对所有 Importing 进行了存储分区或排序，则该联接应转换为存储分区的 Map 端连接或存储分区的排序合并联接。

考虑在不同的键上使用多个 mapjoin 的可能性：

```SQL
select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM
  ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM
    bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)
  ) firstjoin
  JOIN
  smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo)
```

不支持上述查询。如果没有 mapjoin 提示，则以上查询将作为 2 个仅 map 作业执行。如果用户事先知道 Importing 足够小以适合内存，则可以使用以下可配置参数来确保查询在单个 map-reduce 作业中执行。

hive.auto.convert.join.noconditionaltask-Hive 是否启用基于 Importing 文件大小的关于将普通联接转换为 mapjoin 的优化。如果启用此参数，并且 n 向联接的表/分区的 n-1 个大小的总和小于指定的大小，则该联接将直接转换为 mapjoin(没有条件任务)。

hive.auto.convert.join.noconditionaltask.size-如果关闭了 hive.auto.convert.join.noconditionaltask，则此参数不起作用。但是，如果启用了该连接，并且 n 向联接的表/分区的 n-1 个大小的总和小于此大小，则该联接将直接转换为 mapjoin(没有条件任务)。默认值为 10MB。

Hive 会自动识别各种用例并对其进行优化。 Hive 0.11 改进了以下情况的优化程序：

在一侧适合内存的位置连接。在新的优化中：

将该那一侧作为哈希表加载到内存中

只需要扫描较大的表

事实表在内存中的占用空间较小

Star-schema joins

在许多情况下，不再需要提示。

Map 联接由优化器自动拾取。

![JOIN](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/1632465053(1).jpg)

##### 3. 运行层面的调优

**推测执行:**

hive.mapred.reduce.tasks.speculative.execution
默认值：true

添加于：Hive0.5.0

是否应该对减速器进行投机执行。

![推测执行](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150517.bmp)

**并行执行:**

hive.exec.parallel
默认值：false

添加于：Hive0.5.0

是否并行执行作业。适用于可以并行运行的 MapReduce 作业，例如，在连接之前处理不同源表的作业。从Hive 0.14开始，也适用于可并行运行的移动任务，例如，在多插入期间移动文件以插入目标。

hive.exec.parallel.thread.number
默认值：8

添加于：Hive0.6.0

最多可以并行执行多少个作业。

![并行执行](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150835.bmp)

**JVM重用:**

Hadoop与JVM重用对应的参数是mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM。

比如在集群中配置每个slave节点最多同时运行16个map和2个reduce。那slave节点会启动最多16个JVM用于map。

为每个task启动一个新的JVM将耗时1秒左右，对于运行时间较长（比如1分钟以上）的job影响不大，但如果都是时间很短的task，那么频繁启停JVM会有开销。

如果我们想使用JVM重用技术来提高性能，那么可以将mapred.job.reuse.jvm.num.tasks设置成大于1的数。这表示属于同一job的顺序执行的task可以共享一个JVM，也就是说第二轮的map可以重用前一轮的JVM，而不是第一轮结束后关闭JVM，第二轮再启动新的JVM。

那么最多一个JVM能顺序执行多少个task才关闭呢？这个值就是mapred.job.reuse.jvm.num.tasks。如果设置成-1，那么只要是同一个job的task（无所谓多少个），都可以按顺序在一个JVM上连续执行。

如果task属于不同的job，那么JVM重用机制无效，不同job的task需要不同的JVM来运行。

JVM重用技术不是指同一Job的两个或两个以上的task可以同时运行于同一JVM上，而是排队按顺序执行。

一个tasktracker最多可以同时运行的task数目由mapred.tasktracker.map.tasks.maximum和mapred.tasktracker.reduce.tasks.maximum

决定，并且这两个参数在mapred-site.xml中设置。其他方法，如在JobClient端通过命令行-Dmapred.tasktracker.map.tasks.maximum=number或者conf.set("mapred.tasktracker.map.tasks.maximum","number")设置都是无效的。

![jvm优化](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210924150858.bmp)

## 2.Spark

### 2.1.算子

#### 2.1.1.map & mapPartition

分析一下相关的源码，通过这种方式，来分析，二者之间究竟有什么异同。RDD部分代码如下：

```scala
    def map(f: T => U): RDD[U] = withScope {
      val cleanF = sc.clean(f)
      new MapPartitionsRDD(this, (..., iter) => iter.map(cleanF))
    }
```

```scala
    def mapPartitions(f: Iterator[T] => Iterator[U], ...): RDD[U] = withScope {
      val cleanedF = sc.clean(f)
      new MapPartitionsRDD(
        this,
        (..., iter: Iterator[T]) => cleanedF(iter),
        ...)
    }
```

上面的代码比较简单，我们来分析一下：

```scala
可以看到，其实二者返回的都是一个MapPartitionsRDD的实例，传入的第二个参数f有所不同，

而这个f是在RDD的compute中调用的（具体代码后面给出），也就是说这个f是二者的根本区别

我们关注MapPartitionsRDD构造函数的第二个参数：

f: (TaskContext, Int, Iterator) => Iterator map

最后的实参是f = (..., iter) =>iter map cleanedFmapPartitions 的是

f = (..., iter) => cleanedF(iter) cleaned直接当成我们传进来的函数就好，怎么来的这里不关注。

刚才说到，f是compute调用的，而compute是在节点上运行task的时候间接触发的

所以，最后其实就是在单机上，对iter的不同操作方式的区别！

下面我们来想一下，iter map f与f(iter)有什么异同？

前者，会遍历整个iter并且返回相同size的新的迭代器，

而后者可以根据自定义f来操作iter，结果的size可能会发生变化

当然也可能不需遍历整个iter ，比如可能我想返回这个

Iterator( iter.head )如果需要诸如connection的创建

哪个性能高，一下便知二者都不会立即计算，都只能通过

最后的writer.write(rdd.iterator)来触发真正的计算 

没有条件2的情况下，通常性能差别并不大，也通常不会成为瓶颈，没有想象的那么严重

```

![map&mapPartition](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926142704.bmp)

#### 2.1.2.foreach & foreachPartition

基于socket word count写入外部存储mysql ，记录下他们的区别。

```scala

package com.imooc.spark.streaming

import java.sql.DriverManager

import com.imooc.spark.util.ConnectionPool
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * 使用Spark Streaming完成词频统计，并将结果写入到MySQL数据库中
  */
object ForeachRDDApp {

  def main(args: Array[String]): Unit = {

    val sparkConf = new SparkConf().setAppName("ForeachRDDApp").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(5))


    val lines = ssc.socketTextStream("localhost", 6789)

    val result = lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)

    //result.print()  //此处仅仅是将统计结果输出到控制台

    result.foreachRDD(rdd => {
      rdd.foreachPartition(partitionOfRecords => {
        val connection = ConnectionPool.getConnection()
        partitionOfRecords.foreach(record => {
          val sql = "insert into streaming(item, count) values('" + record._1 + "'," + record._2 + ")"
          connection.createStatement().execute(sql)
        })

        ConnectionPool.returnConnection(connection)
      })
    })


    ssc.start()
    ssc.awaitTermination()
  }
}

```

```scala

foreachRDD 源码：


  /**
   * Apply a function to each RDD in this DStream. This is an output operator, so
   * 'this' DStream will be registered as an output stream and therefore materialized.
   */
  def foreachRDD(foreachFunc: RDD[T] => Unit): Unit = ssc.withScope {
    val cleanedF = context.sparkContext.clean(foreachFunc, false)
    foreachRDD((r: RDD[T], _: Time) => cleanedF(r), displayInnerRDDOps = true)
  }

最常用的输出算子，func作用在输出流数据生成的每个RDD,这个函数应该可以把每个RDD中的数据输出到外部系统，
比如存储rdd到文件，通过网络存储到数据库, func 是在运行streming application的driver端进程
注意只有actions算子才会触发RDD的计算。DStream中即使有foreachRDD算子也不会即使进行处理，
只有foreachRDD(func)函数func中存在了action算子才会执行运算，
所以foreachRDD的函数中可以使用foreach和foreachPartition算子来触发action操作。



foreachPartition源码：

/**
 * Applies a function f to each partition of this RDD.
 */
def foreachPartition(f: Iterator[T] => Unit): Unit = withScope {
  val cleanF = sc.clean(f)
  sc.runJob(this, (iter: Iterator[T]) => cleanF(iter))
}

Spark core中的算子
foreachPartition是对每个partition中的iterator实行迭代的处理.
通过用户传入的function（即函数f）对iterator进行内容的处理，
源码中函数f传入的参数是一个迭代器，也就是说在foreachPartiton中函数处理的是分区迭代器，
而非具体的数据


foreach 源码：

/** Applies a function `f` to all values produced by this iterator.
 *
 *  @param  f   the function that is applied for its side-effect to every element.
 *              The result of function `f` is discarded.
 *
 *  @tparam  U  the type parameter describing the result of function `f`.
 *              This result will always be ignored. Typically `U` is `Unit`,
 *              but this is not necessary.
 *
 *  @note    Reuse: $consumesIterator
 *
 *  @usecase def foreach(f: A => Unit): Unit
 *    @inheritdoc
 */
def foreach[U](f: A => U) { while (hasNext) f(next()) }

与foreachPartition类似的是，foreach也是对每个partition中的iterator中的所有值实行迭代处理，
通过用户传入的function（即函数f）对iterator进行内容的处理。
而不同的是，函数f中的参数传入的不再是一个迭代器，而是每次的foreach得到的一个rdd的kv实例，
也就是具体的数据，例如更新一个累加器或与一个外部存储系统交互比如本例中的执行insert操作。
```

1. foreachRDD 是spark streaming 的最常用的output 算子，foreachPartition和foreach 是spark core的算子
2. foreachRDD是执行在driver端，其他两个是执行在exectuor端
3. foreachRDD 输入rdd, 其他两个传入的是iterator, foreachPartition传入的迭代器，foreach传入的是迭代器产生的所有值进行处理，举例说明foreachpartion是每个分区执行一遍，比如说m个分区，n个数据，foreachpartion会执行m次，foreach会执行m*n次

![foreach & foreachPartition](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926144822.png)

#### 2.1.3.reduceByKey & groupByKey

reduceByKey按照key进行聚合，在shuffle之前有预聚合（combine）操作，返回结果为RDD[k,v]。2. groupByKey按照key进行分组后直接shuffle（无预聚合）。预聚合可以提高执行性能，在不影响业务逻辑的情况下，建议优先使用reduceByKey。

- groupByKey 不会在map端进行combine，而reduceByKey 会在map端的默认开启combine进行本地聚合。
- 在map端先进行一次聚合，很极大的减小reduce端的压力，一般来说，map的机器数量是远大于reduce的机器数量的。通过map聚合的方式可以把计算压力平均到各台机器，最终在reduce端只需要汇总map端聚合完成的数据即可，极大的提高效率。

![reduce/group ByKey code](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926151524.png)

![reduce/group ByKey](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926151128.png)

#### 2.1.4.collect

数组 Array 的形式返回数据集的所有元素，具体为将不同分区的数据按照分区顺序采集到Driver端内存中，形成数组。

在驱动程序中，以数组Array的形式返回数据集的所有元素

这个方法可以将RDD类型的数据转化为数组，同时会从远程集群拉取数据到driver端（组成一维数组）。

```scala
import org.apache.spark.{SparkConf, SparkContext}

object action {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    val rdd = sc.makeRDD(List(1,2,3,4),2)
    val ints = rdd.collect()
    println(ints.mkString(","))

    sc.stop()
  }

}

结果：
1,2,3,4
```

![collect](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926152425.png)

#### 2.1.5.coalesce & repartiton

**coalesce**
函数功能：改变原始数据的分区，减少分区数量。coalesce方法默认情况下不会将分区的数据打乱重新组合

有俩个参数：

- numPartitions:（Int） ：设置分区数
- shuffle:（Boolean ）：为Ture时，会进行suffle操作，将之前的分区重新分配，为false时，则不会进行shuffle操作

作用：大量数据进行过滤后，将数据量的小的分区，重新划分为一个分区，提高处理效率。

```scala
package com.atguigu.bigdata.spark.core.RDD.operator.transform

import org.apache.spark.{SparkConf, SparkContext}

object coalesce {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("operator")
    val sc = new SparkContext(sparkConf)
    //filter
    val rdd  = sc.makeRDD(List(1,2,3,4,5,6), 3)
    val newRDD = rdd.coalesce(2)

    newRDD.saveAsTextFile("output")
    sc.stop()

  }

}

实例2:

package com.atguigu.bigdata.spark.core.rdd.operator.transform

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Spark10_RDD_Operator_Transform {

    def main(args: Array[String]): Unit = {

        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
        val sc = new SparkContext(sparkConf)

        // TODO 算子 - filter
        val rdd = sc.makeRDD(List(1,2,3,4,5,6), 3)

        // coalesce方法默认情况下不会将分区的数据打乱重新组合
        // 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜
        // 如果想要让数据均衡，可以进行shuffle处理
        //val newRDD: RDD[Int] = rdd.coalesce(2)
        val newRDD: RDD[Int] = rdd.coalesce(2,true)

        newRDD.saveAsTextFile("output")
        sc.stop()

    }
}
```

**repartiion**
repartition用来调整父RDD的分区数，入参为调整之后的分区数。由于使用方法比较简单，这里就不写例子了。

```scala
  def repartition(numPartitions: Int): DStream[T] = ssc.withScope {
    this.transform(_.repartition(numPartitions))
  }
```

从方法中可以看到，实现repartition的方式是通过Dstream的transform算子之间调用RDD的repartition算子实现的。

接下来就是看看RDD的repartition算子是如何实现的。

```scala
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }

```

首先可以看到RDD的repartition的实现是调用时coalesce方法。其中入参有两个第一个是numPartitions为重新分区后的分区数量，第二个参数为是否shuffle，这里的入参为true代表会进行shuffle。

他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）

1）N<M。一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。

2）如果N>M并且N和M相差不多， (假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false， 在shuffl为false的情况下，如果M>N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。

3）如果N>M并且两者相差悬殊，这时如果将shuffle设置为false，父子RDD是窄依赖关系，他们同处在一个Stage中，就可能造成Spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。

总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDDde分区数变多的。

#### 2.1.6.cache & persist

Spark 中一个很重要的能力是将数据持久化（或称为缓存），在多个操作间都可以访问这些持久化的数据。

当持久化一个 RDD 时，每个节点的其它分区都可以使用 RDD 在内存中进行计算，在该数据上的其他 action 操作将直接使用内存中的数据。

这样会让以后的 action 操作计算速度加快（通常运行速度会加速 10 倍）。

缓存是迭代算法和快速的交互式使用的重要工具。

RDD 可以使用 persist() 方法或 cache() 方法进行持久化。

![cache和persist](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926154947.png)

数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。

在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 persist 方法，Spark 也会自动缓存部分中间数据。

这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法。

每个持久化的 RDD 可以使用不同的存储级别进行缓存

例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。

这些存储级别通过传递一个 StorageLevel 对象给 persist() 方法进行设置。

详细的存储级别介绍如下：

- MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。
- MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。
- MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。
- MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。DISK_ONLY : 只在磁盘上缓存 RDD。
- MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。
- OFF_HEAP）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。

**选择存储级别**
Spark 的存储级别的选择，核心问题是在内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择 :

- 如果使用默认的存储级别（MEMORY_ONLY），存储在内存中的 RDD 没有发生溢出，那么就选择默认的存储级别。默认存储级别可以最大程度的提高 CPU 的效率,可以使在 RDD 上的操作以最快的速度运行。
- 如果内存不能全部存储 RDD，那么使用 MEMORY_ONLY_SER，并挑选一个快速序列化库将对象序列化，以节省内存空间。使用这种存储级别，计算速度仍然很快。
- 除了在计算该数据集的代价特别高，或者在需要过滤大量数据的情况下，尽量不要将溢出的数据存储到磁盘。因为，重新计算这个数据分区的耗时与从磁盘读取这些数据的耗时差不多。
- 如果想快速还原故障，建议使用多副本存储级别（例如，使用 Spark 作为 web 应用的后台服务，在服务出故障时需要快速恢复的场景下）。所有的存储级别都通过重新计算丢失的数据的方式，提供了完全容错机制。但是多副本级别在发生数据丢失时，不需要重新计算对应的数据库，可以让任务继续运行。

### 2.2.序列化

#### 2.2.1.序列化之java序列化

Java序列化: 默认情况下,使用Java的 Spark 序列化对象 ObjectOutputStream 框架,你可以使用任何实现了 java.io.Serializable 的类与它一起使用。你还可以通过继承 java.io.Externalizable 来更深入地控制序列化的性能。Java序列化有着灵活，但通常非常缓慢的特性，会导致许多类的大型序列化格式。

![java序列化操作](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926160954.png)

#### 2.2.2.序列化之Kryo序列化

Kryo serialization: Spark还可以使用Kryo库(version 4)更快地序列化对象。Kryo比Java序列化要快得多(通常多达10倍)，也更紧凑，但它不支持所有的 Serializable 类型，并且要求你预先 注册 将在程序中使用的类，以获得最佳性能。

你可以通过使用 SparkConf 初始化作业并调用 conf.set("spark.serializer", org.apache.spark.serializer.KryoSerializer")，切换使用 Kryo 来进行序列化。此设置配置的序列化程序不仅用于在工作节点之间数据 shuffle，而且还用于将 RDD 序列化到磁盘。Kryo不是默认的 serializer 的唯一原因是自定义的注册要求，但是我们建议在任何网络密集型应用程序中尝试它。自从 Spark 2.0.0 以来，我们在使用简单类型、简单类型数组或字符串类型 RDD 之间 shuffle 时，在内部使用 Kryo serializer。

Spark 默认包含 Kryo 序列化器，用于 Twitter chill 库中 AllScalaRegistrar 涉及的许多常用的Scala核心类。

要向Kryo注册你自己的自定义类，请使用 registerKryoClasses 方法。

```scala

  val conf = new SparkConf().setMaster(...).setAppName(...)
  conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))
  val sc = new SparkContext(conf)

```

如果你的对象很大，你可能还需要增加 spark.kryoserializer.buffer config。这个值需要足够大，以容纳你要序列化的最大对象。

最后，如果你不注册自定义类，Kryo仍然可以工作，但是它必须将完整的类名存储在每个对象中，这是很浪费资源的。

![Kryo](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926161435.png)

![java vs kryo](https://raw.githubusercontent.com/me281026/Img-Repo/master/Img%20Repo/BIG_DATA/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210926161504.png)

### 2.3.流数据处理
